{"cells":[{"cell_type":"markdown","metadata":{"id":"FILifZ7L9ViW"},"source":["# 2023年 大規模言語モデル 第5回演習  テーマ：LLMのFine-Tuning\n","\n","演習目的：LLMで追加学習、評価を行えるようになること"]},{"cell_type":"markdown","metadata":{"id":"o-AUZ_T99ViZ"},"source":["## 目次\n","1. 準備\n","1. Fine-Tuningするモデルについて\n","1. データ準備\n","1. 学習方法\n","1. 評価指標"]},{"cell_type":"markdown","metadata":{"id":"5Ok5DdKo9ViZ"},"source":["## 1. 準備"]},{"cell_type":"markdown","metadata":{"id":"cjEESnZC9ViZ"},"source":["学習の推移を記録するためにwandbをimport"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-e-B6IDP9ViZ","outputId":"de84a2fa-c384-496d-a50f-c6c03cc9d063"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhikaru-n\u001b[0m (\u001b[33mpjt-s\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.15.10 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.9"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/nagazumi/LLM/wandb/run-20230919_130149-wk8epxeu</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/pjt-s/test-stablelm-7b/runs/wk8epxeu' target=\"_blank\">confused-water-5</a></strong> to <a href='https://wandb.ai/pjt-s/test-stablelm-7b' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/pjt-s/test-stablelm-7b' target=\"_blank\">https://wandb.ai/pjt-s/test-stablelm-7b</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/pjt-s/test-stablelm-7b/runs/wk8epxeu' target=\"_blank\">https://wandb.ai/pjt-s/test-stablelm-7b/runs/wk8epxeu</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/pjt-s/test-stablelm-7b/runs/wk8epxeu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f3103f46640>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["#学習状況を可視化\n","import wandb\n","import os\n","#set it manually with the WANDB_NOTEBOOK_NAME environment variable\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"stablelm_qlora.ipynb\"\n","wandb.login()\n","wandb.init(project=\"test-stablelm-7b\")"]},{"cell_type":"markdown","metadata":{"id":"bKByfUBi9Via"},"source":["推論などの時にwarningが出ないようにするためにloggingを設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeXJKEYV9Via"},"outputs":[],"source":["from transformers import logging\n","logging.set_verbosity(logging.CRITICAL)"]},{"cell_type":"markdown","metadata":{"id":"Jet1u38Q9Vib"},"source":["## 2. Fine-Tuningするモデルについて\n","Stability AI Japanから商業利用可能なJapanese StableLM Alphaの提供が開始しました(2023/8/10).  \n","- Japanese StableLM Base Alpha 7B 商用利用可能\n","- Japanese StableLM Instruct Alpha 商業利用不可\n","\n","今回は，Japanese StableLM Base Alpha 7Bを利用し商業利用可能なモデルをFine-Tuningします．"]},{"cell_type":"markdown","metadata":{"id":"a3HxyYMD9Vib"},"source":["まずは，Japanese StableLM Base Alpha 7Bをロードして推論してみましょう  \n","モデル名を指定します"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yjhyv0UV9Vib","executionInfo":{"status":"ok","timestamp":1695202884943,"user_tz":-540,"elapsed":4,"user":{"displayName":"Ryuichi Kawasaki","userId":"13524795365418998603"}}},"outputs":[],"source":["# The model from the Hugging Face hub\n","model_name = \"4bit/japanese-stablelm-base-alpha-7b-s\""]},{"cell_type":"markdown","metadata":{"id":"LeI7yCu79Vic"},"source":["モデルのパラメータを設定する"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"V1VfWSjD9Vic","executionInfo":{"status":"error","timestamp":1695202890574,"user_tz":-540,"elapsed":5634,"user":{"displayName":"Ryuichi Kawasaki","userId":"13524795365418998603"}},"outputId":"866d709d-b1bb-4ee5-82fa-fb5f1bd4d9bf","colab":{"base_uri":"https://localhost:8080/","height":385}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6e5c6eee4017>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# bitsandbytes parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import torch\n","from transformers import BitsAndBytesConfig\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}\n","\n","# set up the model parameters\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","  major, _ = torch.cuda.get_device_capability()\n","  if major >= 8:\n","    print(\"=\" * 80)\n","    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","    print(\"=\" * 80)\n","\n","bnb_config = BitsAndBytesConfig(\n","  load_in_4bit=use_4bit,\n","  bnb_4bit_quant_type=bnb_4bit_quant_type,\n","  bnb_4bit_compute_dtype=compute_dtype,\n","  bnb_4bit_use_double_quant=use_nested_quant,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ABm5CwzR9Vic"},"source":["モデルをロードする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCN8V-Wt9Vic","executionInfo":{"status":"aborted","timestamp":1695202890575,"user_tz":-540,"elapsed":7,"user":{"displayName":"Ryuichi Kawasaki","userId":"13524795365418998603"}}},"outputs":[],"source":["# Load base model\n","from transformers import AutoModelForCausalLM\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map,\n","    trust_remote_code=True\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1"]},{"cell_type":"markdown","metadata":{"id":"L8xK45R19Vic"},"source":["tokenizerをロードする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aa6onfep9Vic","executionInfo":{"status":"aborted","timestamp":1695202890575,"user_tz":-540,"elapsed":7,"user":{"displayName":"Ryuichi Kawasaki","userId":"13524795365418998603"}}},"outputs":[],"source":["# Load LLaMA tokenizer　for japanese stableLM\n","from transformers import LlamaTokenizer\n","tokenizer = LlamaTokenizer.from_pretrained(\"novelai/nerdstash-tokenizer-v1\", additional_special_tokens=['▁▁'])\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"]},{"cell_type":"markdown","metadata":{"id":"FUz_DUc-9Vic"},"source":["モデルに入力するテキストを設定する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lIbqR2_W9Vic","executionInfo":{"status":"aborted","timestamp":1695202890575,"user_tz":-540,"elapsed":7,"user":{"displayName":"Ryuichi Kawasaki","userId":"13524795365418998603"}}},"outputs":[],"source":["B_INST, E_INST = \"[INST]\", \"[/INST]\"\n","# Run text generation pipeline with our next model\n","text = \"西郷隆盛はどんな人物ですか？\"\n","\n","prompt = \"{bos_token}{b_inst} {prompt} {e_inst} \".format(\n","    bos_token=tokenizer.bos_token,\n","    b_inst=B_INST,\n","    prompt=text,\n","    e_inst=E_INST,\n",")\n","print(prompt)\n"]},{"cell_type":"markdown","metadata":{"id":"1uHY72sr9Vid"},"source":["pipeline関数を使いモデルの出力を得る"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_dNmgGt9Vid","executionInfo":{"status":"aborted","timestamp":1695202890576,"user_tz":-540,"elapsed":7,"user":{"displayName":"Ryuichi Kawasaki","userId":"13524795365418998603"}}},"outputs":[],"source":["from transformers import pipeline\n","pipe = pipeline(task=\"text-generation\",\n","                 model=model,\n","                 tokenizer=tokenizer,\n","                 max_new_tokens=30\n","                )\n","result = pipe(prompt)"]},{"cell_type":"markdown","metadata":{"id":"Wfcc_S6K9Vid"},"source":["出力内容を確認"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9IPurQ49Vid","executionInfo":{"status":"aborted","timestamp":1695202890576,"user_tz":-540,"elapsed":7,"user":{"displayName":"Ryuichi Kawasaki","userId":"13524795365418998603"}}},"outputs":[],"source":["print(result[0][\"generated_text\"])\n","#[/INST]以降をprintする\n","\n","print('\\n出力だけを表示')\n","print('-'*80)\n","print(result[0][\"generated_text\"].split(E_INST)[1])\n","print('-'*80)"]},{"cell_type":"markdown","metadata":{"id":"SJVhU6bP9Vid"},"source":["今回の演習ではこのJapanese StableLM Base Alpha 7BをFine-Tuningし,\n","1. 自然な日本語を生成させる\n","1. JGLUEのベンチマークで高いスコアを出す\n","\n","これら２つを目標にFine Tuningとします.   \n","以下のllama2の論文からの画像で学習の流れを示しています.  \n","今回の演習では右下の菱形のSupervised Fine Tuningの部分を実際に動かす流れを確認します.  \n","PretrainingとRLHFの中間の部分です."]},{"cell_type":"markdown","metadata":{"id":"FLKums3g9Vid"},"source":["<p align=\"center\">\n","  <img src=\"./figure_and_table/figure4_llama2.png\" alt=\"404\" width=\"600\"/>\n","</p>\n","\n","[1]の論文から引用"]},{"cell_type":"markdown","metadata":{"id":"XoYmdR-29Vid"},"source":["まずは1.自然な日本語を生成させるためにfine tuningの流れを行います.  \n","その後同様の流れで2.JGLUEでのfine tunigを行います.\n","\n","流れとしては \"データの準備\" -> \"学習\" -> \"評価\" です.  "]},{"cell_type":"markdown","metadata":{"id":"VREo8Kdb9Vid"},"source":["## 3. データ準備"]},{"cell_type":"markdown","metadata":{"id":"oVYLJoJU9Vid"},"source":["### 日本語データはどうやって選ぶ?\n","選ぶポイント  \n","a. データの質  \n","b. 目的に沿った多様なデータ  \n","c. ライセンス  \n","\n","### a.データの質\n","自然な日本語かどうか  \n","翻訳データセットが多いため翻訳の精度の低いと思われるデータセットには注意が必要\n","- kunishou/hh-rlhf-49k-jaから抜粋  \n","    \"ケチャップの汚れはデニムのズボンから出てきますか？\"  \n","    \" Do ketchup stains come out of denim pants? \"\n","\n","### b.目的に沿った多様なデータ\n","目的に沿ったデータを選択.今回は日本語でのQandAが学習できるデータを選択  \n","学習させるデータが特殊なタスクに特化しないようなデータを選択することが重要\n","\n","### c.ライセンス\n","モデルを商業利用可能な状態に保つためには、商業利用可能なデータでの学習が必要\n","\n","### 実際のデータを収集\n","以下のデータは今回使えそうな日本語質問回答のデータセット\n","- kunishou/databricks-dolly-15k-ja (15k)\n","- kunishou/oasst1-89k-ja (55k)\n","- izumi-lab/llm-japanese-dataset (9.05M)\n","- shumpei2525/fine_tuning521k-ja (521k)\n","- sudy-super/CoTangent (100)\n","- livedoor ニュースコーパス\n","\n","以下にhuggingface Datasetの[kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)というデータの画面を示す.  \n","画面から目的、ライセンス、データの質を確認.\n"]},{"cell_type":"markdown","metadata":{"id":"TQDWwesN9Vie"},"source":["<p align=\"center\">\n","  <img src=\"./figure_and_table/HF_databricks-dolly-15k-ja.png\" alt=\"404\" width=\"800\"/>\n","</p>\n","\n","[hugging face](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)のデータセットの画面"]},{"cell_type":"markdown","metadata":{"id":"uYU844E69Vie"},"source":["このデータセットをLoad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hzMx0vS9Vie","outputId":"cc17cbaa-9942-46a5-94f6-98d768ba73e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset json (/home/nagazumi/.cache/huggingface/datasets/kunishou___json/kunishou--databricks-dolly-15k-ja-6459e484ab1ecf5e/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"]}],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"kunishou/databricks-dolly-15k-ja\", split=\"train\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjsHQx4W9Vie","outputId":"d9b2a50c-aa97-47c7-ebb3-d08578fd6eed"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input', 'instruction', 'output', 'index', 'category'],\n","    num_rows: 15015\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#datasetの中身を確認\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiMmaD8H9Vie","outputId":"1f256ba5-ad3a-4993-f9cb-213fdc3083ec"},"outputs":[{"data":{"text/plain":["{'input': '',\n"," 'instruction': 'ラクダはなぜ水なしで長く生きられるのか？',\n"," 'output': 'ラクダは、長時間にわたってエネルギーと水分で満たされた状態を保つために、腰の脂肪を利用しています。',\n"," 'index': '2',\n"," 'category': 'open_qa'}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["dataset[2]"]},{"cell_type":"markdown","metadata":{"id":"nH3DQ7r59Vie"},"source":["演習の時間に制限があるためdatasetを小さくする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drpFs33i9Vie"},"outputs":[],"source":["# datasetの前1000個抽出\n","dataset = dataset.select(range(1000))"]},{"cell_type":"markdown","metadata":{"id":"L9Fze5Bw9Vie"},"source":["datasetを学習時に使える入力文の形に変換します."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAnDBoc69Vie"},"outputs":[],"source":["# プロンプトテンプレートの準備\n","def generate_prompt(data_point):\n","    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n","    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n","    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。質問に答えてください。\"\n","\n","    if data_point[\"input\"]:\n","        text =  f\"\"\"{data_point[\"input\"]}\\n{data_point[\"instruction\"]}\"\"\"\n","    else:\n","        text =  f\"\"\"{data_point[\"instruction\"]}\"\"\"\n","\n","    prompt = \"{bos_token} {b_inst} {prompt} {e_inst} {output}\".format(\n","        bos_token=tokenizer.bos_token,\n","        b_inst=B_INST,\n","        system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n","        prompt=text,\n","        e_inst=E_INST,\n","        output=data_point[\"output\"],\n","    )\n","    return prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJvtk1PY9Vie","outputId":"1ee69a61-79b2-4465-9ede-f59178e50a29"},"outputs":[{"data":{"text/plain":["{'input': '',\n"," 'instruction': 'ラクダはなぜ水なしで長く生きられるのか？',\n"," 'output': 'ラクダは、長時間にわたってエネルギーと水分で満たされた状態を保つために、腰の脂肪を利用しています。',\n"," 'index': '2',\n"," 'category': 'open_qa'}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["dataset[2]"]},{"cell_type":"markdown","metadata":{"id":"dSZBVwuS9Vie"},"source":["datasetとして入らない要素を削除する関数を定義します"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzsjWHkO9Vie"},"outputs":[],"source":["# テキスト列の追加\n","def add_text(example):\n","    example[\"text\"] = generate_prompt(example)\n","    del example[\"index\"]\n","    del example[\"category\"]\n","    del example[\"instruction\"]\n","    del example[\"input\"]\n","    del example[\"output\"]\n","    return example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1VgZvTw9Vil","outputId":"8793b4e2-2fc1-41ab-d247-94c268fb4c03"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/nagazumi/.cache/huggingface/datasets/kunishou___json/kunishou--databricks-dolly-15k-ja-6459e484ab1ecf5e/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4cd2ba916a27d44c.arrow\n"]}],"source":["dataset = dataset.map(add_text)"]},{"cell_type":"markdown","metadata":{"id":"cwlhSUis9Vil"},"source":["Datasetがpromptの形式に変換できているか確認"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BARZGfsZ9Vil","outputId":"b6bd49b0-8bea-4076-e44c-dd08a172ccff"},"outputs":[{"name":"stdout","output_type":"stream","text":["------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","{'text': '<|startoftext|> [INST] ラクダはなぜ水なしで長く生きられるのか？ [/INST] ラクダは、長時間にわたってエネルギーと水分で満たされた状態を保つために、腰の脂肪を利用しています。'}\n","------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"]}],"source":["print('-'*180)\n","print(dataset[2])\n","print('-'*180)"]},{"cell_type":"markdown","metadata":{"id":"97f_yqKp9Vil"},"source":["## 4.学習方法"]},{"cell_type":"markdown","metadata":{"id":"2QmvblS19Vil"},"source":["### 学習手法の説明\n","\n","今回の演習ではPEFTの手法の中でAdapter型の手法LoRA, QLoRAの実装を紹介する.  \n","\n","### LoRA\n","LoRA(Low-Rank Adaptation)は以下のようにTransformerの中間層に行列を追加し  \n","追加した層のパラメタのみを学習する.   "]},{"cell_type":"markdown","metadata":{"id":"KLNWFV5B9Vil"},"source":["<p align=\"center\">\n","  <img src=\"./figure_and_table/AACL_2022_tutorial_PLMs_LoRA.png\" alt=\"404\" width=\"600\"/>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"Nk9-Y6O_9Vil"},"source":["この追加した箇所は以下の図のように並列に追加される.\n"]},{"cell_type":"markdown","metadata":{"id":"ZjojzoMv9Vil"},"source":["<p align=\"center\">\n","  <img src=\"./figure_and_table/figure1_lora.png\" alt=\"404\" width=\"200\"/>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"y_QXkVwu9Vil"},"source":["実装は以下のようになる.  \n","\n","```python\n","input_dim = 768  # 入力次元d\n","output_dim = 768  # 出力次元d\n","\n","rank = 8  # The rank 'r' for the low-rank adaptation パラメタの次元\n","\n","W = ... # 事前学習で学習したFeed-Forwardのパラメタ　次元は input_dim x output_dim\n","\n","W_A = nn.Parameter(torch.empty(input_dim, rank)) # LoRA weight A 次元は input_dim x rank\n","W_B = nn.Parameter(torch.empty(rank, output_dim)) # LoRA weight B 次元は rank x output_dim\n","\n","alpha = 1.0 # 追加した層の結果を調整するハイパーパラメタ\n","...\n","\n","# 通常のFeed-Forwardの実装\n","def regular_forward_matmul(x, W):\n","    h = x @ W\n","return h\n","\n","# LoRAの実装\n","def lora_forward_matmul(x, W, W_A, W_B):\n","    h = x @ W  # regular matrix multiplication\n","    h += x @ (W_A @ W_B)*alpha # alphaで追加した層の出力を調整するハイパーパラメタ\n","return h\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"P1bjKG8A9Vil"},"source":["メリット\n","- 学習の速度向上：学習時更新するパラメタが減るため学習時間が早くなる.(推論は高速化しない)  \n","- メモリ使用量の削減：学習時更新するパラメタが減る.  \n","上記のプログラムを例にすると学習するパラメタのオーダーは W_A,W_B の合計で$2*d*rank$となる.  \n","そのため、保持しておくメモリ使用量が減る.(推論は小メモリ化しない)  \n","\n","使用するメモリの概算  \n","- モデルサイズ：7b\n","- パラメタサイズ：float16(16ビット浮動小数点数 詳しくは[torch.flaot16](https://note.nkmk.me/python-pytorch-dtype-to/))  \n","\n","総メモリ:7b * 16bit / 8  = 14GB  \n","\n","\n","14GBのモデルサイズで学習するとバッチサイズ4とすると14GB*4 = 56GBのメモリが必要.  \n","\n","LoRA実装リンク:[peft](https://github.com/huggingface/peft)  \n"]},{"cell_type":"markdown","metadata":{"id":"2RVwh5B_9Vim"},"source":["LoRAパラメータを設定する  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7fJgcxh9Vim"},"outputs":[],"source":["from peft import LoraConfig, PeftModel\n","################################################################################\n","# LoRA parameters\n","################################################################################\n","\n","# LoRA rank dimension\n","lora_r = 64\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"query_key_value\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"YYjB2cZX9Vim"},"source":["LoRAでFine-Tuningするためには56GBのGPUメモリが必要.  \n","このメモリが乗る場合はOKだが乗らない場合バッチサイズを減らすか、学習方法を変える必要がある.  \n","演習環境は24GB,(Google colab無料枠17GB)であるため、LoRAを量子化したQLoRAという学習手法を紹介する."]},{"cell_type":"markdown","metadata":{"id":"SNygi93X9Vim"},"source":["ここでの量子化(Quantize)とはfloat型(16 bit)で扱っていたパラメタをそれよりの小さい型(8 bitや4 bit)で扱うこと."]},{"cell_type":"markdown","metadata":{"id":"BEJL96Ln9Vim"},"source":["---\n","### QLoRA\n","\n","QLoRAのポイントは以下の3つ\n","- 4bit NormalFloat: 64個のパラメタ群を標準化して4bitで表現する,  \n","→パラメタ64個が4bit, 64個のパラメタ群の量子化定数が32bitでそれぞれ表現される.   \n","量子化前：32bit * 64 = 2048bit  \n","量子化後： ４bit * 64 + 32bit = 288bit\n","\n","- Double Quantization: 4bit NormalFloatで表現された量子化定数群を256個ごとにさらに量子化する\n","→量子化定数が8bit, 256個のパラメタ群の量子化定数が32bit,でそれぞれ表現される.  \n","量子化前：４bit * 64 + (32bit) = 288bit  \n","量子化後：４bit * 64 + (8bit + 32bit * 64/256) = 272bit\n","\n","\n","- Paged Optimizers\n","CPUにメモリの一部を乗せておくことでGPU内のメモリ使用量を削減する.\n","\n","これらの手法によりメモリ使用量を削減することができ学習に必要なメモリを概算すると以下のようになる.  \n","7b * 4bit / 8  = 3.5GB  \n","\n","3.5GBのモデルサイズで学習するとバッチサイズ4とすると3.5GB*4 = 14GBのメモリが必要.  \n","colab無料枠は17GBであるため、学習可能\n","\n","量子化実装リンク:[bitsandbytes](https://github.com/timdettmers/bitsandbytes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fgi86zaf9Vim"},"outputs":[],"source":["from transformers import BitsAndBytesConfig\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ofFlAWfA9Vim"},"source":["学習パラメタの設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvU-ANjc9Vim"},"outputs":[],"source":["# 学習に使うパラメータを設定します\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","# Number of training epochs\n","num_train_epochs = 1\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = True\n","bf16 = False\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-5\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.01\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","# Learning rate schedule (constant a bit better than cosine)\n","lr_scheduler_type = \"constant\"\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","# Save checkpoint every X updates steps\n","save_steps = 50\n","# Log every X updates steps\n","logging_steps = 50"]},{"cell_type":"markdown","metadata":{"id":"DUHROYqt9Vim"},"source":["SFTのパラメタを設定する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bgz-nVA9Vim"},"outputs":[],"source":["################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdb7-sNJ9Vim","outputId":"87fa4a03-acc8-4c3a-dfda-4171eb078284"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","Your GPU supports bfloat16: accelerate training with bf16=True\n","================================================================================\n"]},{"name":"stderr","output_type":"stream","text":["/home/nagazumi/anaconda3/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n","/home/nagazumi/anaconda3/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Loading cached processed dataset at /home/nagazumi/.cache/huggingface/datasets/kunishou___json/kunishou--databricks-dolly-15k-ja-6459e484ab1ecf5e/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-19aff4628778064f.arrow\n"]}],"source":["#学習に使うパラメータを元にTrainerを作成\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"wandb\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")"]},{"cell_type":"markdown","metadata":{"id":"n6iLVX4-9Vim"},"source":["実際にトレーニングを行う"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Z-mak6H9Vim","outputId":"283576eb-33f9-4d73-b8ef-464a6a677ddb"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'loss': 2.6394, 'learning_rate': 2e-05, 'epoch': 0.2}\n","{'loss': 2.3556, 'learning_rate': 2e-05, 'epoch': 0.4}\n","{'loss': 2.2746, 'learning_rate': 2e-05, 'epoch': 0.6}\n","{'loss': 2.2077, 'learning_rate': 2e-05, 'epoch': 0.8}\n","{'loss': 2.2361, 'learning_rate': 2e-05, 'epoch': 1.0}\n","{'train_runtime': 218.8989, 'train_samples_per_second': 4.568, 'train_steps_per_second': 1.142, 'train_loss': 2.342683288574219, 'epoch': 1.0}\n"]}],"source":["# Train model\n","trainer.train()\n","# Fine-tuned model name\n","new_model = \"japanese-stablelm-7b-miniguanaco\"\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"markdown","metadata":{"id":"2U1trnM39Vim"},"source":["テストとして学習"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTAyVZCL9Vim","outputId":"05c5a8a6-35ff-44eb-c591-3df70326b8fa"},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset json (/home/nagazumi/.cache/huggingface/datasets/kunishou___json/kunishou--databricks-dolly-15k-ja-6459e484ab1ecf5e/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['input', 'instruction', 'output', 'index', 'category'],\n","    num_rows: 100\n","})"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["dataset_test = load_dataset(\"kunishou/databricks-dolly-15k-ja\", split=\"train\")\n","dataset_test = dataset_test.select(range(1000,1100))\n","dataset_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dCRp25Y9Vin","outputId":"f85e7b63-da75-4a84-9798-19ef7fcc289a"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|startoftext|> [INST] 夏は、伝統的に暑い、あるいは暖かい天候を連想させます。地中海沿岸の気候では乾燥した天候を意味し、その他の地域では（特にモンスーンのある東アジアでは）雨の天候を意味する。雨季は、サバンナ気候の中で植生が成長する主要な期間です。雨季が偏西風の季節的な変化と関連している場合、モンスーンと呼ばれる。\n","大西洋北部では、6月1日から11月30日まで、熱帯低気圧の季節があります。大西洋のハリケーン・シーズンの統計的なピークは9月10日である。北東太平洋では、より広い活動期間がありますが、大西洋と同じような時間枠です。北西太平洋では、2月と3月に最小となり、9月上旬にピークを迎える熱帯低気圧が一年中見られます。北インド海盆では、4月から12月に嵐が最も多く、5月と11月にピークがあります。南半球では、熱帯低気圧のシーズンは11月初旬から4月末までで、ピークは2月中旬から3月上旬です。\n","アメリカやカナダでは、春から夏にかけて雷雨のシーズンがありますが、秋には10月や11月まで続くこともあります。これらの嵐は雹、強風、竜巻を発生させることがあり、通常は午後から夕方にかけて発生します。\n","北インド海盆で嵐が多いのはいつ頃ですか？ [/INST] 北インド盆地では、4月から12月までが最も多く、5月と11月にピークがあります。\n"]},{"data":{"text/plain":["'<|startoftext|> [INST] 夏は、伝統的に暑い、あるいは暖かい天候を連想させます。地中海沿岸の気候では乾燥した天候を意味し、その他の地域では（特にモンスーンのある東アジアでは）雨の天候を意味する。雨季は、サバンナ気候の中で植生が成長する主要な期間です。雨季が偏西風の季節的な変化と関連している場合、モンスーンと呼ばれる。\\n大西洋北部では、6月1日から11月30日まで、熱帯低気圧の季節があります。大西洋のハリケーン・シーズンの統計的なピークは9月10日である。北東太平洋では、より広い活動期間がありますが、大西洋と同じような時間枠です。北西太平洋では、2月と3月に最小となり、9月上旬にピークを迎える熱帯低気圧が一年中見られます。北インド海盆では、4月から12月に嵐が最も多く、5月と11月にピークがあります。南半球では、熱帯低気圧のシーズンは11月初旬から4月末までで、ピークは2月中旬から3月上旬です。\\nアメリカやカナダでは、春から夏にかけて雷雨のシーズンがありますが、秋には10月や11月まで続くこともあります。これらの嵐は雹、強風、竜巻を発生させることがあり、通常は午後から夕方にかけて発生します。\\n北インド海盆で嵐が多いのはいつ頃ですか？ [/INST] '"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["test_prompt = generate_prompt(dataset_test[1])\n","print(test_prompt)\n","# test_promptの[/INST]以降の部分を削除\n","test_prompt = test_prompt.split(\"[/INST] \")[0]+\"[/INST] \"\n","test_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLghhjEe9Vin","outputId":"1f784735-4575-4fc2-88d0-a27a2b9c573e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|startoftext|> [INST] 夏は、伝統的に暑い、あるいは暖かい天候を連想させます。地中海沿岸の気候では乾燥した天候を意味し、その他の地域では（特にモンスーンのある東アジアでは）雨の天候を意味する。雨季は、サバンナ気候の中で植生が成長する主要な期間です。雨季が偏西風の季節的な変化と関連している場合、モンスーンと呼ばれる。\n","大西洋北部では、6月1日から11月30日まで、熱帯低気圧の季節があります。大西洋のハリケーン・シーズンの統計的なピークは9月10日である。北東太平洋では、より広い活動期間がありますが、大西洋と同じような時間枠です。北西太平洋では、2月と3月に最小となり、9月上旬にピークを迎える熱帯低気圧が一年中見られます。北インド海盆では、4月から12月に嵐が最も多く、5月と11月にピークがあります。南半球では、熱帯低気圧のシーズンは11月初旬から4月末までで、ピークは2月中旬から3月上旬です。\n","アメリカやカナダでは、春から夏にかけて雷雨のシーズンがありますが、秋には10月や11月まで続くこともあります。これらの嵐は雹、強風、竜巻を発生させることがあり、通常は午後から夕方にかけて発生します。\n","北インド海盆で嵐が多いのはいつ頃ですか？ [/INST] \n"]}],"source":["print(test_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGtqHZL_9Vin","outputId":"f4f9c2e0-82a6-4469-c7d1-9351bbf7b22d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/nagazumi/anaconda3/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"]}],"source":["pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=50)\n","result = pipe(test_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3LoOkD09Vin","outputId":"66a073f1-694d-4da9-af91-5523eff264cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|startoftext|> [INST] 夏は、伝統的に暑い、あるいは暖かい天候を連想させます。地中海沿岸の気候では乾燥した天候を意味し、その他の地域では（特にモンスーンのある東アジアでは）雨の天候を意味する。雨季は、サバンナ気候の中で植生が成長する主要な期間です。雨季が偏西風の季節的な変化と関連している場合、モンスーンと呼ばれる。\n","大西洋北部では、6月1日から11月30日まで、熱帯低気圧の季節があります。大西洋のハリケーン・シーズンの統計的なピークは9月10日である。北東太平洋では、より広い活動期間がありますが、大西洋と同じような時間枠です。北西太平洋では、2月と3月に最小となり、9月上旬にピークを迎える熱帯低気圧が一年中見られます。北インド海盆では、4月から12月に嵐が最も多く、5月と11月にピークがあります。南半球では、熱帯低気圧のシーズンは11月初旬から4月末までで、ピークは2月中旬から3月上旬です。\n","アメリカやカナダでは、春から夏にかけて雷雨のシーズンがありますが、秋には10月や11月まで続くこともあります。これらの嵐は雹、強風、竜巻を発生させることがあり、通常は午後から夕方にかけて発生します。\n","北インド海盆で嵐が多いのはいつ頃ですか？ [/INST] インド洋の嵐のピークは、11月中旬から12月上旬です。\n","[/INST] 夏は、伝統的に暑い、あるいは暖かい天候を連想させます。地中海沿岸の気候では\n"]}],"source":["print(result[0][\"generated_text\"])"]},{"cell_type":"markdown","metadata":{"id":"1wKOSOdt9Vin"},"source":["## 5. 評価指標\n","### 日本語ベンチマークJGLUEによる評価でスコアを上げるためのFine tuning\n","\n","- JSQuAD  \n","    文章読解テスト:文脈として情報が与え、質問をする.文脈内に答えが必ず存在する"]},{"cell_type":"markdown","metadata":{"id":"yFjn7Eoa9Vin"},"source":["流れとしては\n","1. 数字で評価するための関数を定義する\n","1. 学習前にスコア計算\n","1. 学習後\n","1. 学習後のスコア計算"]},{"cell_type":"markdown","metadata":{"id":"X--FL3PD9Vin"},"source":["<|startoftext|>[INST] [題名]:梅雨  \n","[問題]:梅雨（つゆ、ばいう）は、北海道と小笠原諸島を除く日本、朝鮮半島南部、中国の南部から長江流域にかけての沿海部、および台湾など、東アジアの 広範囲においてみられる特有の気象現象で、5月から7月にかけて来る曇りや雨の多い期間のこと。雨季の一種である。  \n","[質問]:梅雨がみられるのはどの期間？  \n"," [/INST][答え]: 5月から7月にかけて   \n","\n","このような形式で学習、検証をしていきます"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsEI7va29Vin"},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","import random\n","import string\n","import re\n","from pprint import pprint\n","from tqdm.notebook import tqdm\n","\n","import emoji\n","import neologdn\n","\n","from datasets import load_dataset\n","import datasets\n","datasets.disable_progress_bar()\n","\n","from collections import Counter\n","from tenacity import (\n","    retry,\n","    stop_after_attempt,\n","    wait_random_exponential,\n",")"]},{"cell_type":"markdown","metadata":{"id":"zl_PY_tP9Vin"},"source":["スコアを出す関数を定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIOplaTI9Vin"},"outputs":[],"source":["# @title 指標計算などに関する関数を定義\n","\n","def normalize(s):\n","    \"\"\"\n","    Normalize a sentence by removing punctuations and emoji.\n","\n","    Parameters\n","    ----------\n","    s : str\n","    Sentence.\n","\n","    Returns\n","    ----------\n","    Normalized sentence.\n","    \"\"\"\n","    def remove_punc(tokens):\n","        exclude = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n","        exclude += string.punctuation\n","        exclude = [*exclude]\n","        return \"\".join([tok for tok in tokens if tok not in exclude])\n","\n","    def remove_emoji(text):\n","        text = \"\".join([\"\" if emoji.is_emoji(c) else c for c in text])\n","        emoji_pattern = re.compile(\n","            \"[\"\n","            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","            u\"\\U00002702-\\U000027B0\"\n","            \"]+\",\n","            flags=re.UNICODE,\n","        )\n","        return emoji_pattern.sub(r\"\", text)\n","\n","    return remove_punc(neologdn.normalize(remove_emoji(s)))\n","\n","# Accuracy算出（完全一致）\n","def accuracy(prediction: str, ground_truth: str):\n","    \"\"\"\n","    Calculate exact-match accuracy.\n","\n","    Parameters\n","    ----------\n","    prediction : str\n","    Sentence predicted by a model.\n","\n","    ground_truth : str\n","    Sentence of ground truth.\n","\n","    Returns\n","    ----------\n","    Boolean. True if prediction is exactly equal to the ground truth after normalization.\n","    \"\"\"\n","    return normalize(prediction) == normalize(ground_truth)\n","\n","# F1スコア算出\n","def f1(prediction: str, ground_truth: str):\n","    \"\"\"\n","    Calculate F1 score by character.\n","\n","    Parameters\n","    ----------\n","    prediction : str\n","    Sentence predicted by a model.\n","\n","    ground_truth : str\n","    Sentence of ground truth.\n","\n","    Returns\n","    ----------\n","    F1 score.\n","\n","    Note\n","    ----------\n","    Calculate this score by character because the result will depend on morphogen parser chosen if by word/token.\n","    ref. https://fintan.jp/page/9126/、https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/H9-4.pdf\n","    \"\"\"\n","    prediction_chars = [char for char in normalize(prediction)]\n","    ground_truth_chars = [char for char in normalize(ground_truth)]\n","\n","    common = Counter(prediction_chars) & Counter(ground_truth_chars)\n","    num_same = sum(common.values())\n","\n","    if num_same == 0:\n","        return 0\n","    precision = num_same / len(prediction_chars)\n","    recall = num_same / len(ground_truth_chars)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","# 予測値と正解のデータセットに対して指標計算を走らせる\n","def calculate_score(score_func, predictions: list, ground_truths: list):\n","    \"\"\"\n","    Calculate metrics for datasets of predictions and ground_truths.\n","\n","    Parameters\n","    ----------\n","    predictions : list\n","    List of sentence predicted by a model.\n","\n","    ground_truths : list\n","    List of sentence of ground truth.\n","\n","    Returns\n","    ----------\n","    Score(Metrics).\n","    \"\"\"\n","    score = total = 0\n","    for pred, gtruth in zip(predictions, ground_truths):\n","        total += 1\n","        if isinstance(gtruth, list): #JSQuADは質問毎に解答が3つ存在するので、その中で最もスコアが大きいものを採用する(lm-evaluation-harness-jp-stableと同様の手法)\n","            stored_scores = []\n","            for gt in gtruth:\n","                sc = score_func(pred, gt)\n","                stored_scores.append(sc)\n","            score += max(stored_scores)\n","        else:\n","          score += score_func(pred, gtruth)\n","\n","    return score / total"]},{"cell_type":"markdown","metadata":{"id":"JUG1U13E9Vin"},"source":["JSQuADの形式を入力に変換する関数を定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZ0tgFCD9Vin"},"outputs":[],"source":["\n","import time\n","# JSQuAD: 文脈に基づいて質問に回答する（問題文から答えを抜き出す）\n","\"\"\"\n","次のコードを参考にしている\n","　- https://github.com/tdc-yamada-ya/lm-evaluation-harness-jp-stable/blob/jp-stable/lm_eval/tasks/jsquad.py\n","\"\"\"\n","\n","class JSQuAD():\n","    DESCRIPTION_JSQUAD = \"[題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい。\\n\\n\"\n","    SEED = 42\n","\n","    def __init__(self):\n","        ds = load_dataset(\"shunk031/JGLUE\", name=\"JSQuAD\")\n","        self.ds_train = list(map(self.process_doc, ds['train']))\n","        self.ds_valid = list(map(self.process_doc, ds['validation']))\n","        self.metrics = [accuracy, f1] # 完全一致によるAccuracy、および、文字単位でのF1スコアで評価する\n","\n","    def process_doc(self, doc):\n","        return {\n","            \"title\": doc[\"title\"],\n","            \"context\": doc[\"context\"],\n","            \"question\": doc[\"question\"],\n","            \"answers\": doc[\"answers\"][\"text\"]\n","        }\n","\n","    def doc_to_text(self, doc):\n","        B_INST, E_INST = \"[INST]\", \"[/INST]\"\n","        B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n","        DESCRIPTION_JSQUAD = \"[題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい\"\n","        text = f\"[題名]:{doc['title']}\\n\"\\\n","            + f\"[問題]:{doc['context'].split('[SEP]')[-1].strip()}\\n\"\\\n","            + f\"[質問]:{doc['question']}\\n\"\n","\n","        prompt = \"{bos_token}{b_inst} {prompt} {e_inst} \".format(\n","            bos_token=tokenizer.bos_token,\n","            b_inst=B_INST,\n","            system=f\"{B_SYS}{DESCRIPTION_JSQUAD}{E_SYS}\",\n","            prompt=text,\n","            e_inst=E_INST+ \"[答え]: \"# + doc[\"answers\"][\"text\"][0],\n","        )\n","        return prompt\n","\n","    def doc_to_answer(self, doc) -> list:\n","        return doc['answers']\n","\n","    def fewshot_examples(self, num_fewshot=0, seed=SEED):\n","        if num_fewshot == 0:\n","            labeled_examples = \"\"\n","        else:\n","            #trainデータからランダムにnum_fewshotの数だけサンプル抽出\n","            fewshotex = random.Random(seed).sample(self.ds_train, num_fewshot)\n","\n","            labeled_examples = (\n","                \"-------------\\n\"\n","                + \"[例]:\\n\"\n","                + \"\\n\\n\".join(\n","                    [\n","                        self.doc_to_text(doc) + self.doc_to_answer(doc)[0] #doc_to_answer()のoutputはリストであることに注意\n","                        for doc in fewshotex\n","                    ]\n","                )\n","                + \"\\n-------------\"\n","                + \"\\n\\n\"\n","            )\n","\n","        return labeled_examples\n","\n","    def get_query(self, doc, num_fewshot=0, description=DESCRIPTION_JSQUAD, seed=SEED):\n","        return (\n","            description\n","            + self.fewshot_examples(num_fewshot=num_fewshot, seed=seed)\n","            + self.doc_to_text(doc)\n","        )"]},{"cell_type":"markdown","metadata":{"id":"tGQWhSgS9Vio"},"source":["スコア関数を使い全体の評価を計算する関数を定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Phw12eER9Vio"},"outputs":[],"source":["# @title 評価指標の出力に関する関数を定義\n","\n","# ランダムサンプリングして小さなデータセットを作成する\n","def make_small_data(data, n, seed=42):\n","    \"\"\"\n","    Create small dataset.\n","\n","    Parameters\n","    ----------\n","    data : list\n","\n","    n : int\n","    Number of data to be extracted by random sampling.\n","\n","    seed : int\n","    Random seed.\n","\n","    Returns\n","    ----------\n","    small_data : list\n","    List of data.\n","    \"\"\"\n","    random.Random(seed).shuffle(data)\n","    small_data = data[:n]\n","\n","    return small_data\n","\n","\n","# 特定のモデル・タスク・shot数（0,1,...）に対する評価指標を算出する\n","def evaluate(chat_func, num_fewshot, task_name):\n","    \"\"\"\n","    Evaluate LLM on specific task such as JCommonsenseQA and JSQuAD.\n","\n","    Parameters\n","    ----------\n","    chat_func : function\n","\n","    num_fewshot : int\n","    Number of examples to be added to query.\n","\n","    task_name : str\n","    Task to evaluate LLM such as \"JCommonsenseQA\" and \"JSQuAD\".\n","\n","    Returns\n","    ----------\n","    scores : dictionary\n","    Dictionary of scores for the task.\n","    \"\"\"\n","\n","    N = 8 #モデル評価に用いるvalidationデータ数\n","\n","    task_dict = {\n","        #'JCommonsenseQA': JCommonsenseQA,\n","        'JSQuAD': JSQuAD\n","    } #対応可能なタスクの一覧\n","\n","    # タスクを設定\n","    try:\n","        task = task_dict[task_name]()\n","    except KeyError:\n","        print(\"Available tasks:\")\n","        pprint([t for t in task_dict.keys()])\n","        raise KeyError(f\"Missing task {task_name}\")\n","\n","    # データセットの読み込み\n","    ds_valid = make_small_data(task.ds_valid, N) #計算時間短縮のため、精度検証にはvalidationデータからN件抽出して用いる\n","\n","    # LLMに投げる質問と正解の出力\n","    queries = list(map(task.get_query, ds_valid, [num_fewshot]*len(ds_valid)))\n","    answers = list(map(task.doc_to_answer, ds_valid))\n","\n","    # LLMの回答を出力\n","    responses = list(map(chat_func, tqdm(queries, leave=False)))\n","\n","    # 評価指標の算出と出力\n","    scores = {}\n","    for score_func in task.metrics:\n","        score = calculate_score(score_func, responses, answers)\n","        print(responses, answers)\n","        scores[score_func.__name__] = score\n","\n","    return scores\n","\n","\n","# 指定したzero/few-shot、および、タスクの条件に従い、評価指標の算出結果を出力する\n","def get_evaluation_results(chat_func,\n","                           shot_mode: list=['zero', 'few'],\n","                           task_list: list=['JSQuAD']):\n","                           #task_list: list=['JCommonsenseQA', 'JSQuAD']):\n","    \"\"\"\n","    Print out the result of evaluation for LLM.\n","\n","    Parameters\n","    ----------\n","    chat_func : function\n","\n","    shot_mode : list\n","    List that can only have \"zero\" or \"few\" as the element.\n","    Choose \"zero\" when you want to evaluate model with zero-shot and \"few\" when do with few-shot.\n","\n","    task_list : list\n","    Task list you may want to evaluate LLM.\n","    \"\"\"\n","    NUM_FEWSHOT = 1\n","\n","    print(\"========================================\")\n","    print(\"[LLMの定量評価結果]\")\n","    print(f\"生成関数（モデル）: {chat_func.__name__}\")\n","\n","    for mode in shot_mode:\n","        assert mode in ['zero', 'few'], 'List of shot_mode should only include \"zero\" or \"few\".'\n","        num_fewshot = 0 if mode == 'zero' else NUM_FEWSHOT\n","\n","        print(f\"{mode}-shot(例の数{num_fewshot}個)による実行結果：\")\n","\n","        for i, task in enumerate(task_list):\n","            print(f\" {i+1}.{task}: {evaluate(chat_func, num_fewshot, task)}\")\n","\n","        print()\n","    print(\"========================================\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOcNcqnH9Vio","outputId":"03c60a71-8a9d-4bd9-da24-c5a593b8ad6c","colab":{"referenced_widgets":["161fd132834940dbb873ee0c80c20256"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"161fd132834940dbb873ee0c80c20256","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map,\n","    trust_remote_code=True\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1"]},{"cell_type":"markdown","metadata":{"id":"GlbmGpXO9Vio"},"source":["modelの回答を出力する関数を定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9PqhGaq9Vio"},"outputs":[],"source":["# @title modelの回答を出力する関数\n","\n","# modelによる回答を出力する\n","def run_model(question, model=model):\n","  #inputs = tokenizer(question, return_tensors=\"pt\").input_ids.cuda()\n","  pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=50)\n","  result = pipe(question)\n","  result = result[0][\"generated_text\"]\n","  # 質問文も含めて出力してしまうため、一旦\"\\n[答え]:\"の後を回答として出力する\n","  response_text = result.split('[答え]:')[-1]\n","\n","  response_text = response_text.split('\\n')[0]\n","  print(\"##############\")\n","  print(response_text)\n","  return response_text"]},{"cell_type":"markdown","metadata":{"id":"vsiApKhX9Vio"},"source":["学習前に評価を実行"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83AG6WH29Vio","outputId":"d90e474c-b3fc-4921-9258-065e29a25b64","colab":{"referenced_widgets":["c288c004df5749dd89ee8bf2b5761872","fae3c03a710b4fcb9e432e959827bb4c"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["========================================\n","[LLMの定量評価結果]\n","生成関数（モデル）: run_model\n","zero-shot(例の数0個)による実行結果：\n"]},{"name":"stderr","output_type":"stream","text":["Found cached dataset jglue (/home/nagazumi/.cache/huggingface/datasets/shunk031___jglue/JSQuAD/1.1.0/1ced89c0df5e89a46cc50032fa7ff73ace35706e86a6d533de4b222334115262)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c288c004df5749dd89ee8bf2b5761872","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/nagazumi/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["##############\n","  [建築]と[土木]のダブルディグリーの機会を提供する新しい教育制度\n","##############\n","5月から7月にかけて\n","##############\n","  [INST] [題名]:ポルトガル\n","##############\n","  [問題]:この改元は改元の決定から日付までを幕府側の事実上の指定で決められたものである。これは事実上の将軍の代始改元を目指したものであるが、以後は幕府権力の衰退もありこれが最後となった。\n","##############\n","  [RIAI]\n","##############\n","  [題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい。.\n","##############\n","  [題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい。.\n","##############\n","  [INST] [題名]:ラオス\n","['  [建築]と[土木]のダブルディグリーの機会を提供する新しい教育制度', '5月から7月にかけて', '  [INST] [題名]:ポルトガル', '  [問題]:この改元は改元の決定から日付までを幕府側の事実上の指定で決められたものである。これは事実上の将軍の代始改元を目指したものであるが、以後は幕府権力の衰退もありこれが最後となった。', '  [RIAI]', '  [題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい。.', '  [題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい。.', '  [INST] [題名]:ラオス'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n","['  [建築]と[土木]のダブルディグリーの機会を提供する新しい教育制度', '5月から7月にかけて', '  [INST] [題名]:ポルトガル', '  [問題]:この改元は改元の決定から日付までを幕府側の事実上の指定で決められたものである。これは事実上の将軍の代始改元を目指したものであるが、以後は幕府権力の衰退もありこれが最後となった。', '  [RIAI]', '  [題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい。.', '  [題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい。.', '  [INST] [題名]:ラオス'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n"," 1.JSQuAD: {'accuracy': 0.25, 'f1': 0.3200119169220293}\n","\n","few-shot(例の数1個)による実行結果：\n"]},{"name":"stderr","output_type":"stream","text":["Found cached dataset jglue (/home/nagazumi/.cache/huggingface/datasets/shunk031___jglue/JSQuAD/1.1.0/1ced89c0df5e89a46cc50032fa7ff73ace35706e86a6d533de4b222334115262)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fae3c03a710b4fcb9e432e959827bb4c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["##############\n","  建築家\n","##############\n","  5月から7月にかけて\n","##############\n","  リスボン、ポルト、ファロ\n","##############\n","  徳川吉宗\n","##############\n","  RIAI\n","##############\n","  住居番号\n","##############\n","  グスタフ・マーラー\n","##############\n","  19世紀後半\n","['  建築家', '  5月から7月にかけて', '  リスボン、ポルト、ファロ', '  徳川吉宗', '  RIAI', '  住居番号', '  グスタフ・マーラー', '  19世紀後半'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n","['  建築家', '  5月から7月にかけて', '  リスボン、ポルト、ファロ', '  徳川吉宗', '  RIAI', '  住居番号', '  グスタフ・マーラー', '  19世紀後半'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n"," 1.JSQuAD: {'accuracy': 0.375, 'f1': 0.465625}\n","\n","========================================\n"]}],"source":["get_evaluation_results(run_model, shot_mode=['zero', 'few'])"]},{"cell_type":"markdown","metadata":{"id":"yL3oP-g79Vip"},"source":["次に先ほどの評価指標についてスコアを上げることを目的として学習の流れをもう一度確認します."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcRzUIV79Vip","outputId":"abae7fa7-a883-4800-d41c-bdaeab417151"},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset jglue (/home/nagazumi/.cache/huggingface/datasets/shunk031___jglue/JSQuAD/1.1.0/1ced89c0df5e89a46cc50032fa7ff73ace35706e86a6d533de4b222334115262)\n"]}],"source":["#shunk031/JGLUEからJSQuADのデータセットを読み込む    スコア算出ではvalidation(4.44k rows)を使用するのでtrinを使用\n","dataset = load_dataset(\"shunk031/JGLUE\", \"JSQuAD\", split=\"train\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hW95v_ZW9Vip","outputId":"c7e5d3a7-8379-4d4e-b91f-920722f41e1c"},"outputs":[{"data":{"text/plain":["{'id': 'a1000888p0q2',\n"," 'title': '造語',\n"," 'context': '造語 [SEP] 造語（ぞうご）は、新たに語（単語）を造ることや、既存の語を組み合わせて新たな意味の語を造ること、また、そうして造られた語である。新たに造られた語については、新語または新造語とも呼ばれる。',\n"," 'question': 'たに語（単語）を造ることや、既存の語を組み合わせて新たな意味の語を造ること、また、そうして造られた語を意味する言葉は？',\n"," 'answers': {'text': ['造語'], 'answer_start': [0]},\n"," 'is_impossible': False}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["dataset[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPos8YKj9Vip","outputId":"b37c9036-4404-4a9d-fb8c-109b5edb6dd7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/nagazumi/.cache/huggingface/datasets/shunk031___jglue/JSQuAD/1.1.0/1ced89c0df5e89a46cc50032fa7ff73ace35706e86a6d533de4b222334115262/cache-75d6426c86e6b2d0.arrow\n"]}],"source":["def generate_prompt(data_point):\n","    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n","    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n","    DESCRIPTION_JSQUAD = \"[題名]と[問題]から[質問]に対する[答え]を名詞で抜き出しなさい\"\n","    text = f\"[題名]:{data_point['title']}\\n\"\\\n","        + f\"[問題]:{data_point['context'].split('[SEP]')[-1].strip()}\\n\"\\\n","        + f\"[質問]:{data_point['question']}\\n\"\n","\n","    prompt = \"{bos_token}{b_inst} {prompt} {e_inst} \".format(\n","        bos_token=tokenizer.bos_token,\n","        b_inst=B_INST,\n","        system=f\"{B_SYS}{DESCRIPTION_JSQUAD}{E_SYS}\",\n","        prompt=text,\n","        e_inst=E_INST+ \"[答え]: \" + data_point[\"answers\"]['text'][0],\n","    )\n","    return prompt\n","\n","# テキスト列の追加\n","def add_text(example):\n","    example[\"text\"] = generate_prompt(example)\n","    del example[\"id\"]\n","    del example[\"title\"]\n","    del example[\"context\"]\n","    del example[\"question\"]\n","    del example[\"answers\"]\n","    del example[\"is_impossible\"]\n","    return example\n","dataset = dataset.map(add_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HKQ5ULG9Vip","outputId":"0aa1eed0-efe9-437d-daf4-e57fc247d0f6"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text'],\n","    num_rows: 62859\n","})"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIk_b4lC9Vip","outputId":"66cf0c28-5c04-4ef9-d529-06fd38dd7401"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|startoftext|>[INST] [題名]:造語\n","[問題]:造語（ぞうご）は、新たに語（単語）を造ることや、既存の語を組み合わせて新たな意味の語を造ること、また、そうして造られた語である。新たに造られた語については、新語または新造語とも呼ばれる。\n","[質問]:たに語（単語）を造ることや、既存の語を組み合わせて新たな意味の語を造ること、また、そうして造られた語を意味する言葉は？\n"," [/INST][答え]: 造語 \n"]}],"source":["print(dataset[2]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WErzl9cv9Vip"},"outputs":[],"source":["#学習時間を短くするために、データセットを1000程度に件に絞る\n","dataset_small = dataset.select(range(0,1000))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enxVVFQf9Vip","outputId":"360b1791-6ae4-46ad-b344-92f58cb04502"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text'],\n","    num_rows: 1000\n","})"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["dataset_small"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhLHQ65H9Vip","outputId":"e8c550f9-c840-4427-8fe0-e243206c94ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/nagazumi/anaconda3/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["/home/nagazumi/anaconda3/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Loading cached processed dataset at /home/nagazumi/.cache/huggingface/datasets/shunk031___jglue/JSQuAD/1.1.0/1ced89c0df5e89a46cc50032fa7ff73ace35706e86a6d533de4b222334115262/cache-c6ec7a2eb32f670f.arrow\n"]}],"source":["# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset_small,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUaziaFI9Vip","outputId":"0c7a1f60-d1dc-413e-89f2-ac5caad93b95"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'loss': 2.7509, 'learning_rate': 2e-05, 'epoch': 0.2}\n","{'loss': 2.2118, 'learning_rate': 2e-05, 'epoch': 0.4}\n","{'loss': 1.9653, 'learning_rate': 2e-05, 'epoch': 0.6}\n","{'loss': 1.935, 'learning_rate': 2e-05, 'epoch': 0.8}\n","{'loss': 1.8956, 'learning_rate': 2e-05, 'epoch': 1.0}\n","{'train_runtime': 173.8936, 'train_samples_per_second': 5.751, 'train_steps_per_second': 1.438, 'train_loss': 2.151725006103516, 'epoch': 1.0}\n"]}],"source":["# Fine-tuned model name\n","jglue_model = \"japanese-stablelm-7b-jglue\"\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(jglue_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6pkclpZd9Vip","outputId":"489b9dd4-f30c-46e2-a8c7-7d8df53143c2","colab":{"referenced_widgets":["d109d41c2ece40bc974c8161bef3f05f","1dd1a7e06d7e4edcabcc989528786ba6"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["========================================\n","[LLMの定量評価結果]\n","生成関数（モデル）: run_model\n","zero-shot(例の数0個)による実行結果：\n"]},{"name":"stderr","output_type":"stream","text":["Found cached dataset jglue (/home/nagazumi/.cache/huggingface/datasets/shunk031___jglue/JSQuAD/1.1.0/1ced89c0df5e89a46cc50032fa7ff73ace35706e86a6d533de4b222334115262)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d109d41c2ece40bc974c8161bef3f05f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/nagazumi/anaconda3/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"]},{"name":"stdout","output_type":"stream","text":["##############\n"," ダブルディグリーの\n","##############\n","5月から7月にかけて\n","##############\n","  [INST]:ポルトガル\n","##############\n","  [徳川吉宗]\n","##############\n","  RIAI\n","##############\n","  フロンテージ [/INST][問題]: 街区周辺を市町村の中心に近い角を起点にし、そこから街区の外周に沿って時計回りに距離を測って10 m（ - 15 m）ごとに区切り\n","##############\n","  グスタフ・マーラー\n","##############\n","  [問題]:このような標高による住み分け分布ができたのは、紀元前からモン・クメール系の人々がこの地域に暮らしていたが、9世紀頃からタイ系の人々が南下してきた。その後、清代末期の19世紀後半からモン\n","[' ダブルディグリーの', '5月から7月にかけて', '  [INST]:ポルトガル', '  [徳川吉宗]', '  RIAI', '  フロンテージ [/INST][問題]: 街区周辺を市町村の中心に近い角を起点にし、そこから街区の外周に沿って時計回りに距離を測って10\\xa0m（ - 15\\xa0m）ごとに区切り', '  グスタフ・マーラー', '  [問題]:このような標高による住み分け分布ができたのは、紀元前からモン・クメール系の人々がこの地域に暮らしていたが、9世紀頃からタイ系の人々が南下してきた。その後、清代末期の19世紀後半からモン'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n","[' ダブルディグリーの', '5月から7月にかけて', '  [INST]:ポルトガル', '  [徳川吉宗]', '  RIAI', '  フロンテージ [/INST][問題]: 街区周辺を市町村の中心に近い角を起点にし、そこから街区の外周に沿って時計回りに距離を測って10\\xa0m（ - 15\\xa0m）ごとに区切り', '  グスタフ・マーラー', '  [問題]:このような標高による住み分け分布ができたのは、紀元前からモン・クメール系の人々がこの地域に暮らしていたが、9世紀頃からタイ系の人々が南下してきた。その後、清代末期の19世紀後半からモン'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n"," 1.JSQuAD: {'accuracy': 0.25, 'f1': 0.334967751313636}\n","\n","few-shot(例の数1個)による実行結果：\n"]},{"name":"stderr","output_type":"stream","text":["Found cached dataset jglue (/home/nagazumi/.cache/huggingface/datasets/shunk031___jglue/JSQuAD/1.1.0/1ced89c0df5e89a46cc50032fa7ff73ace35706e86a6d533de4b222334115262)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1dd1a7e06d7e4edcabcc989528786ba6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["##############\n","  建築家としても土木技師としてもダブルディグリーの機会を提供する新しい教育制度\n","##############\n","  5月から7月にかけて来る曇りや雨の多い期間\n","##############\n","  リスボン、ポルト、ファロ\n","##############\n","  徳川吉宗\n","##############\n","  RIAI\n","##############\n","  フロンテージ\n","##############\n","  グスタフ・マーラー\n","##############\n","  19世紀後半\n","['  建築家としても土木技師としてもダブルディグリーの機会を提供する新しい教育制度', '  5月から7月にかけて来る曇りや雨の多い期間', '  リスボン、ポルト、ファロ', '  徳川吉宗', '  RIAI', '  フロンテージ', '  グスタフ・マーラー', '  19世紀後半'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n","['  建築家としても土木技師としてもダブルディグリーの機会を提供する新しい教育制度', '  5月から7月にかけて来る曇りや雨の多い期間', '  リスボン、ポルト、ファロ', '  徳川吉宗', '  RIAI', '  フロンテージ', '  グスタフ・マーラー', '  19世紀後半'] [['教育', '教育', '教育制度'], ['5月から7月にかけて', '5月から7月にかけて', '5月から7月'], ['リスボン、ポルト、ファロ', 'リスボン、ポルト、ファロ', '空港'], ['将軍', '将軍', '将軍'], ['RIAI', 'RIAI', 'RIAI'], ['フロンテージ', 'フロンテージ', 'フロンテージ'], ['シェーンベルク', 'シェーンベルク', 'シェーンベルク'], ['9世紀頃', '9世紀頃', '9世紀頃']]\n"," 1.JSQuAD: {'accuracy': 0.375, 'f1': 0.5700796850998464}\n","\n","========================================\n"]}],"source":["get_evaluation_results(run_model, shot_mode=['zero', 'few'])"]},{"cell_type":"markdown","metadata":{"id":"m4V-qw2T9Vip"},"source":["## 参考文献\n","[[1]](https://arxiv.org/abs/2307.09288)Hugo Touvron, Thomas Scialom, et al. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.\tarXiv:2307.09288\n","\n","[[2]](https://arxiv.org/abs/2106.09685)Edward J. Hu, et al. (2021) \"LoRA: Low-Rank Adaptation of Large Language Models\" Edward J. Hu, et al. arXiv:2106.09685\n","\n","[[3]](https://arxiv.org/abs/2305.14314)Tim Dettmers, et al. (2023) \"QLoRA: Efficient Finetuning of Quantized LLMs\" arXiv:2305.14314\n","\n","[[4]](https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial/lecture_material/AACL_2022_tutorial_PLMs.pdf) Cheng-Han Chiang et al. (2023) \"Recent Advances in Pre-trained Language Models:\n","Why Do They Work and How to Use Them\"\n","\n","[[5]](https://lightning.ai/pages/community/article/lora-llm/)Sebastian Raschka (2023) \"Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)\"\n","\n","[[6]](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) Maxime Labonne (2023) \"Fine-Tune Your Own Llama 2 Model in a Colab Notebook A practical introduction to LLM fine-tuning\""]},{"cell_type":"markdown","metadata":{"id":"jFetigzI9Vip"},"source":["追記\n","コードのmodel_nameとトークナイザを変更すれば他のモデルでも利用可能です"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VarqYTpB9Viq"},"outputs":[],"source":["#\"elyza/ELYZA-japanese-Llama-2-7b\"\n","model_name = \"elyza/ELYZA-japanese-Llama-2-7b\"\n","# load tokenizer for ELYZA-japanese-Llama-2-7b\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}