{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023年 大規模言語モデル サマースクール 第6回演習\n",
    "\n",
    "## 目次\n",
    "1. [RLHFの概要と実装するためのライブラリについて](##1.-RLHFの概要と実装するためのライブラリについて)\n",
    "2. [使用するデータセットの形式](##2.-使用するデータセットの形式)\n",
    "3. [報酬モデルの学習](##3.-報酬モデルの学習)\n",
    "4. [PPOを用いて強化学習を行う](##4.-PPOを用いて強化学習を行う)\n",
    "5. [参考文献](##5.-参考文献)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-feXdy31kBh"
   },
   "source": [
    "## 1. RLHFの概要と実装するためのライブラリについて\n",
    "\n",
    "### RLHFの概要\n",
    "人間のフィードバックからの強化学習(RLHF)は、人間の価値基準に沿うように、人間のフィードバックを使ってAI（言語）モデルを強化学習で微調整（ファインチューニング）する手法である。ChatGPTにも用いられている技術であり，学習方法の概要は以下の通りである．\n",
    "\n",
    "\n",
    "RLHFの学習方法 ([source](https://arxiv.org/abs/2203.02155)): \n",
    "1. 事前学習されたモデルをファインチューニング\n",
    "2. 人間によるランキングをもとに報酬モデルを学習\n",
    "3. 学習された報酬モデルを用いてPPOで強化学習を行う\n",
    "\n",
    "![](https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg)\n",
    "\n",
    "### trlxについて\n",
    "「trlX」 (Transformer Reinforcement Learning X)は、「報酬を計算する関数」または「ラベル付きのデータセット(ex. HH-RLHF)」のいずれかを使用して、強化学習で大規模言語モデル (LLM) をファインチューニングするために分散学習フレームワークです。\n",
    "「facebook/opt-6.7b」「EleutherAI/gpt-neox-20b」など、最大200億のパラメータの「causal」および「T5」ベースの言語モデルをファインチューニングできます。\n",
    "現在、次の強化学習アルゴリズムが実装されています。\n",
    "\n",
    "```\n",
    "・PPO (Proximal Policy Optimization)\n",
    "・ILQL (Implicit Language Q-Learning)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54TgX16_1s9Z"
   },
   "source": [
    "## 2. 使用するデータセットの形式\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"The quick brown fox...\",\n",
    "    \"answer1\": \"jumps over the lazy dog.\",\n",
    "    \"answer2\": \"bags few lynx.\",\n",
    "}\n",
    "```\n",
    "\n",
    "Labelerは、プロンプトが表示されたときに、どの選択が好ましいかをフィードバックします。人間のLabelerによるこのランキングによって、報酬モデルを学習します。\n",
    "\n",
    "この例では、prompt, answer1(good), answer2(bad)の3つがある辞書のリストでデータセットを定義しています。  \n",
    "(Labelerはanswer1 > answer2とランキング)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer1': 'Let the spotlight shine on something big, something that matters. '\n",
      "            \"If you haven't picked up on this year's stocks market (which will \"\n",
      "            'likely be over for a few months), then you may be missing',\n",
      " 'answer2': 'Today the U.S. stock market rose for the 10th consecutive year '\n",
      "            'and for the ninth consecutive year to trade at their highest '\n",
      "            'level since January 2004.\\n'\n",
      "            '\\n'\n",
      "            '\"These were big gains',\n",
      " 'prompt': 'What is the latest news on the stock market?'}\n",
      "'--------------------------------------------------'\n",
      "{'chosen': 'TL;DR:  Snooped, found something, should I admit what I found so '\n",
      "           'we can have a more honest conversation about it with less denial '\n",
      "           'on her part?',\n",
      " 'prompt': 'SUBREDDIT: r/relationships\\n'\n",
      "           'TITLE: To admit or not to admit snooping...\\n'\n",
      "           'POST: I [25M] have snooped in the past and copped up to it to my '\n",
      "           'gf [25F] of 6 years.  We talked it through.  It had been a year or '\n",
      "           \"two since the last time.  That's an issue I'm working on.\\n\"\n",
      "           '\\n'\n",
      "           \"Now she has a new close male work friend.  I won't go into \"\n",
      "           'details, but she hides things from me with him and does other '\n",
      "           'things to make me a bit suspicious.  So...I snooped again, and '\n",
      "           'this time, all texts from her new friend have been deleted and I '\n",
      "           'saw a google search for \"how to get over a guy\" near some searches '\n",
      "           'of his name and views of his Facebook profile.\\n'\n",
      "           '\\n'\n",
      "           'I asked her about this guy, not mentioning the snooping, and she '\n",
      "           'denied any feelings, we talked for a long time about our '\n",
      "           'relationship and she insisted that she only loves me and I mean '\n",
      "           'the world to her, and that she really wants to work towards '\n",
      "           \"getting this relationship back out of the rut we've been in (we \"\n",
      "           'both work all the time and barely see each other).\\n'\n",
      "           '\\n'\n",
      "           'I think if I cop to the snooping, we might have a more honest '\n",
      "           \"conversation about what's actually going on (if something is) and \"\n",
      "           \"why she's having these feelings so we can either work through it \"\n",
      "           'together (my preference) or move on.  But obviously, it will open '\n",
      "           \"the pandora's box of the snooping.\\n\"\n",
      "           '\\n'\n",
      "           \"Think it's worth it to admit to the snooping to hopefully get to \"\n",
      "           'the bottom of this?',\n",
      " 'rejected': 'TL;DR:  I snooped, we talked about it, she wants to work it out, '\n",
      "             \"I'm not sure.  Is the snooping worth it?\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import codecs\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# サンプルデータが保存されているパス\n",
    "data_path = 'input_data.json'\n",
    "\n",
    "with codecs.open(data_path, 'r', encoding='utf-8') as f:\n",
    "      data = json.load(f)\n",
    "pprint(data[0])\n",
    "\n",
    "# 既存のデータセットを使用する場合\n",
    "pprint(\"-\" * 50)\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=['train[:100]', 'test[:100]'])\n",
    "pprint(dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfFUYE34atPL"
   },
   "source": [
    "## 3. 報酬モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 報酬モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 報酬モデルを定義するクラス\n",
    "class GPTRewardModel(nn.Module):\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        # 事前学習済みのGPTモデルをロード\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        self.config = model.config\n",
    "        # `gpt-neo(x)` モデルは `hidden_size` 属性名を使っているので、それに合わせて設定\n",
    "        self.config.n_embd = self.config.hidden_size if hasattr(self.config, \"hidden_size\") else self.config.n_embd\n",
    "        self.transformer = model.transformer\n",
    "        # スカラーの出力に対する線形層を設定\n",
    "        self.v_head = nn.Linear(self.config.n_embd, 1, bias=False)\n",
    "        # トークナイザを初期化\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.PAD_ID = self.tokenizer(self.tokenizer.pad_token)[\"input_ids\"][0]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        mc_token_ids=None,\n",
    "        labels=None,\n",
    "        return_dict=False,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ):\n",
    "        loss = None\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        \n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        rewards = self.v_head(hidden_states).squeeze(-1)\n",
    "        chosen_end_scores = []\n",
    "        rejected_end_scores = []\n",
    "\n",
    "        # 入力と報酬を「chosen」と「rejected」に分ける\n",
    "        assert len(input_ids.shape) == 2\n",
    "        bs = input_ids.shape[0] // 2\n",
    "        chosen = input_ids[:bs]\n",
    "        rejected = input_ids[bs:]\n",
    "        chosen_rewards = rewards[:bs]\n",
    "        rejected_rewards = rewards[bs:]\n",
    "\n",
    "        loss = 0\n",
    "        inference = False\n",
    "        for i in range(bs):\n",
    "            # ２つのシーケンスが同じ場合\n",
    "            if torch.all(torch.eq(chosen[i], rejected[i])).item():\n",
    "                c_inds = (chosen[i] == self.PAD_ID).nonzero()\n",
    "                c_ind = c_inds[0].item() if len(c_inds) > 0 else chosen.shape[1]\n",
    "                chosen_end_scores.append(chosen_rewards[i, c_ind - 1])\n",
    "                inference = True\n",
    "                continue\n",
    "\n",
    "            # パディングが存在するかどうかをチェック\n",
    "            c_inds = (chosen[i] == self.PAD_ID).nonzero()\n",
    "            c_ind = c_inds[0].item() if len(c_inds) > 0 else chosen.shape[1]\n",
    "            r_inds = (rejected[i] == self.PAD_ID).nonzero()\n",
    "            r_ind = r_inds[0].item() if len(r_inds) > 0 else rejected.shape[1]\n",
    "            end_ind = max(c_ind, r_ind)\n",
    "\n",
    "            # 選ばれたシーケンスと拒否されたシーケンスが異なる最初のインデックスを取得\n",
    "            divergence_ind = (chosen[i] != rejected[i]).nonzero()[0]\n",
    "            assert divergence_ind > 0\n",
    "\n",
    "            # 対応する報酬にインデックスを適用\n",
    "            c_truncated_reward = chosen_rewards[i][divergence_ind:end_ind]\n",
    "            r_truncated_reward = rejected_rewards[i][divergence_ind:end_ind]\n",
    "\n",
    "            # 最後の報酬をリストに追加\n",
    "            chosen_end_scores.append(c_truncated_reward[-1])\n",
    "            rejected_end_scores.append(r_truncated_reward[-1])\n",
    "\n",
    "            # 損失を計算\n",
    "            loss += -torch.log(torch.sigmoid(c_truncated_reward - r_truncated_reward)).mean()\n",
    "        loss = loss / bs\n",
    "\n",
    "        if not inference:\n",
    "            chosen_end_scores = torch.stack(chosen_end_scores)\n",
    "            rejected_end_scores = torch.stack(rejected_end_scores)\n",
    "\n",
    "        if inference:\n",
    "            chosen_end_scores = torch.stack(chosen_end_scores)\n",
    "            return {\"chosen_end_scores\": chosen_end_scores}\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"chosen_end_scores\": chosen_end_scores,\n",
    "            \"rejected_end_scores\": rejected_end_scores,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6bu2klzkXwie"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-22 03:51:10,093] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# chosenとrejectedの文章ペアをロードする関数\n",
    "def create_comparison_dataset_ls(path: str):\n",
    "    with codecs.open(data_path, 'r', encoding='utf-8') as f:\n",
    "          data = json.load(f)\n",
    "    pairs = []\n",
    "    for sample in data:\n",
    "        chosen = None\n",
    "        rejected = None\n",
    "        pair = {\n",
    "            'chosen': sample['answer1'],\n",
    "            'rejected': sample['answer2']\n",
    "        }\n",
    "        pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "# 文章ペアをtokenizeしてデータセット化\n",
    "class PairwiseDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length):\n",
    "        self.chosen_input_ids = []\n",
    "        self.chosen_attn_masks = []\n",
    "        self.rejected_input_ids = []\n",
    "        self.rejected_attn_masks = []\n",
    "        for pair in tqdm(pairs):\n",
    "            chosen, rejected = pair[\"chosen\"], pair[\"rejected\"]\n",
    "            chosen_encodings_dict = tokenizer(\n",
    "                \"<|startoftext|>\" + chosen + \"<|endoftext|>\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            rejected_encodings_dict = tokenizer(\n",
    "                \"<|startoftext|>\" + rejected + \"<|endoftext|>\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.chosen_input_ids.append(chosen_encodings_dict[\"input_ids\"])\n",
    "            self.chosen_attn_masks.append(chosen_encodings_dict[\"attention_mask\"])\n",
    "            self.rejected_input_ids.append(rejected_encodings_dict[\"input_ids\"])\n",
    "            self.rejected_attn_masks.append(rejected_encodings_dict[\"attention_mask\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chosen_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.chosen_input_ids[idx],\n",
    "            self.chosen_attn_masks[idx],\n",
    "            self.rejected_input_ids[idx],\n",
    "            self.rejected_attn_masks[idx],\n",
    "        )\n",
    "\n",
    "\n",
    "# 報酬モデルに入力できるようにバッチ化する関数\n",
    "# chosen, rejectedのデータを連結する\n",
    "class DataCollatorReward:\n",
    "    def __call__(self, data):\n",
    "        batch = {}\n",
    "        batch[\"input_ids\"] = torch.cat([f[0] for f in data] + [f[2] for f in data])\n",
    "        batch[\"attention_mask\"] = torch.cat([f[1] for f in data] + [f[3] for f in data])\n",
    "        batch[\"labels\"] = torch.tensor([0] * len(data) + [1] * len(data))\n",
    "        return batch\n",
    "\n",
    "\n",
    "# 正解率を算出する関数\n",
    "# chosenデータの方がスコアが高いペアの割合を正解率にしている\n",
    "def compute_metrics(eval_preds):\n",
    "    chosen_end_scores = eval_preds.predictions[0]  # chosen scores\n",
    "    rejected_end_scores = eval_preds.predictions[1]  # rejected scores\n",
    "\n",
    "    result = {}\n",
    "    acc = sum(chosen_end_scores > rejected_end_scores) / len(rejected_end_scores)\n",
    "    result[\"accuracy\"] = acc\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 データセットと報酬モデルの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': 'Let the spotlight shine on something big, something that matters. '\n",
      "           \"If you haven't picked up on this year's stocks market (which will \"\n",
      "           'likely be over for a few months), then you may be missing',\n",
      " 'rejected': 'Today the U.S. stock market rose for the 10th consecutive year '\n",
      "             'and for the ninth consecutive year to trade at their highest '\n",
      "             'level since January 2004.\\n'\n",
      "             '\\n'\n",
      "             '\"These were big gains'}\n"
     ]
    }
   ],
   "source": [
    "# データセットペア取得\n",
    "pairs = create_comparison_dataset_ls(data_path)\n",
    "pprint(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 885.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   27,    91,  9688,  1659,  5239,    91,    29,  5756,   262, 17838,\n",
       "          18340,   319,  1223,  1263,    11,  1223,   326,  6067,    13,  1002,\n",
       "            345,  4398,   470,  6497,   510,   319,   428,   614,   338, 14420,\n",
       "           1910,   357,  4758,   481,  1884,   307,   625,   329,   257,  1178,\n",
       "           1933,   828,   788,   345,   743,   307,  4814, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[   27,    91,  9688,  1659,  5239,    91,    29,  8888,   262,   471,\n",
       "             13,    50,    13,  4283,  1910,  8278,   329,   262,   838,   400,\n",
       "          12785,   614,   290,   329,   262, 19646, 12785,   614,   284,  3292,\n",
       "            379,   511,  4511,  1241,  1201,  3269,  5472,    13,   198,   198,\n",
       "              1,  4711,   547,  1263,  8810, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの動作確認\n",
    "# 以下の値が返却される\n",
    "# (\n",
    "#     chosen_input_id,\n",
    "#     chosen_attn_mask,\n",
    "#     rejected_input_id,\n",
    "#     rejected_attn_mask,\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = PairwiseDataset(pairs, tokenizer, max_length=550)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   27,    91,  9688,  ..., 50256, 50256, 50256],\n",
       "         [   27,    91,  9688,  ..., 50256, 50256, 50256],\n",
       "         [   27,    91,  9688,  ..., 50256, 50256, 50256],\n",
       "         ...,\n",
       "         [   27,    91,  9688,  ..., 50256, 50256, 50256],\n",
       "         [   27,    91,  9688,  ..., 50256, 50256, 50256],\n",
       "         [   27,    91,  9688,  ..., 50256, 50256, 50256]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 報酬モデルに入力できる形にバッチ化する\n",
    "data_collator = DataCollatorReward()\n",
    "data = data_collator([dataset[i] for i in range(5)])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(1.0065, grad_fn=<DivBackward0>),\n",
       " 'chosen_end_scores': tensor([-5.5683, -7.3404, -7.2044, -8.6393, -7.2811], grad_fn=<StackBackward0>),\n",
       " 'rejected_end_scores': tensor([-7.1448, -6.5397, -7.1639, -7.1666, -7.0192], grad_fn=<StackBackward0>)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 報酬モデルへデータを入力\n",
    "# 各文章ごとにスコアが計算される\n",
    "model = GPTRewardModel(\"gpt2\")\n",
    "model(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ハイパラを設定し，実際に報酬モデルを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rC8CG9c1Ze5o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 912.11it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 935.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# GPT-2用のtokenizerを初期化\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# チェックポイントのディレクトリが存在しない場合、新たに作成\n",
    "if not os.path.exists(\"rm_checkpoint\"):\n",
    "    os.mkdir(\"rm_checkpoint\")\n",
    "\n",
    "# 報酬モデルを初期化\n",
    "model = GPTRewardModel(\"gpt2\")\n",
    "\n",
    "# 報酬モデルのトランスフォーマの最初の70%の層を凍結\n",
    "layers = model.transformer.h\n",
    "num_layers = len(layers)\n",
    "num_unfrozen = int(0.3 * num_layers)\n",
    "for layer in layers[:-num_unfrozen]:\n",
    "    layer.requires_grad_(False)\n",
    "\n",
    "# データセットペアをロード\n",
    "pairs = create_comparison_dataset_ls(data_path)\n",
    "# 80%を訓練データ，20%を検証データとして分割\n",
    "train_size = int(0.8 * len(pairs))\n",
    "train_pairs = pairs[0:train_size]\n",
    "val_pairs = pairs[train_size:]\n",
    "\n",
    "# 訓練と検証用のデータセットを作成\n",
    "max_length = 550\n",
    "train_dataset = PairwiseDataset(train_pairs, tokenizer, max_length=max_length)\n",
    "val_dataset = PairwiseDataset(val_pairs, tokenizer, max_length=max_length)\n",
    "\n",
    "data_collator = DataCollatorReward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "6YkKOleIcoQF",
    "outputId": "2e69b9d4-97ba-4ee5-878f-84684d4d2b6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseele\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/llm-seminar/notebooks/day6/wandb/run-20230922_035125-d6ve6j8y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seele/huggingface/runs/d6ve6j8y' target=\"_blank\">gallant-durian-10</a></strong> to <a href='https://wandb.ai/seele/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seele/huggingface' target=\"_blank\">https://wandb.ai/seele/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seele/huggingface/runs/d6ve6j8y' target=\"_blank\">https://wandb.ai/seele/huggingface/runs/d6ve6j8y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:52, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.885200</td>\n",
       "      <td>0.812972</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.858500</td>\n",
       "      <td>0.792872</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.747300</td>\n",
       "      <td>0.776134</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>0.782413</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>0.827006</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.7337837409973145, metrics={'train_runtime': 117.2659, 'train_samples_per_second': 10.66, 'train_steps_per_second': 0.426, 'total_flos': 0.0, 'train_loss': 0.7337837409973145, 'epoch': 50.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"rm_checkpoint/\",\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_accumulation_steps=1,\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ").train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5KzwZ0lifpX"
   },
   "source": [
    "## 4. PPOを用いて強化学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZmGb3nMRu1-V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import trlx.data\n",
    "from trlx.data.configs import (\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    SchedulerConfig,\n",
    "    TokenizerConfig,\n",
    "    TrainConfig,\n",
    "    TRLConfig,\n",
    ")\n",
    "from trlx.models.modeling_ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 学習した報酬モデルをロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTRewardModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (v_head): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REWARD_CHECKPOINT_PATH = \"./rm_checkpoint/checkpoint-50/pytorch_model.bin\"\n",
    "SFT_MODEL_PATH = \"gpt2\"\n",
    "\n",
    "rw_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "rw_tokenizer.pad_token = rw_tokenizer.eos_token\n",
    "rw_model = GPTRewardModel(SFT_MODEL_PATH)\n",
    "rw_model.load_state_dict(torch.load(REWARD_CHECKPOINT_PATH))\n",
    "rw_model.half()\n",
    "rw_model.eval()\n",
    "rw_device = torch.device(\"cuda:{}\".format(0))  # set reward model device\n",
    "rw_model.to(rw_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 必要な関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(samples):\n",
    "    scores_list = []\n",
    "    batch_size = 2\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        sub_samples = samples[i : i + batch_size]\n",
    "        sub_samples = [\"<|startoftext|>\" + chosen + \"<|endoftext|>\" for chosen in sub_samples]\n",
    "        encodings_dict = rw_tokenizer(\n",
    "            sub_samples,\n",
    "            truncation=True,\n",
    "            max_length=config.train.seq_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encodings_dict[\"input_ids\"].to(rw_device)\n",
    "        attn_masks = encodings_dict[\"attention_mask\"].to(rw_device)\n",
    "        input_ids = input_ids.repeat(2, 1)\n",
    "        attn_masks = attn_masks.repeat(2, 1)\n",
    "        with torch.no_grad():\n",
    "            sub_scores = rw_model(input_ids=input_ids, attention_mask=attn_masks)\n",
    "        scores_list.append(sub_scores[\"chosen_end_scores\"])\n",
    "    scores = torch.cat(scores_list, dim=0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_prompt_dataset(prompts, max_length):\n",
    "    formatted_prompts = []\n",
    "    for i in tqdm(range(len(prompts))):\n",
    "        tmp = tokenizer.decode(\n",
    "            tokenizer(\n",
    "                prompts[i].split(\"TL;DR:\")[0],\n",
    "                truncation=True,\n",
    "                max_length=max_length - 5, \n",
    "                add_special_tokens=False,\n",
    "            )[\"input_ids\"],\n",
    "            skip_special_tokens=True,\n",
    "        ).strip()\n",
    "        tmp = tmp + \"\\nTL;DR:\"\n",
    "        tmp = tokenizer.decode(\n",
    "            tokenizer(tmp, truncation=True, max_length=max_length, add_special_tokens=False)[\"input_ids\"],\n",
    "            skip_special_tokens=True,\n",
    "        ).strip()\n",
    "        formatted_prompts.append(tmp)\n",
    "    return formatted_prompts\n",
    "\n",
    "def reward_fn(samples, **kwargs):\n",
    "    original_samples = [text.split(\"TL;DR:\")[0] + \"TL;DR: \" for text in samples]\n",
    "    original_samples = [text + post_summary_dict[text.strip()] for text in original_samples]\n",
    "    original_scores = get_scores(original_samples)\n",
    "    scores = get_scores(samples)\n",
    "    norms_scores = scores - original_scores\n",
    "    return norms_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ハイパラを設定し，実際に報酬モデルを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 150.89it/s]\n",
      "100%|██████████| 500/500 [00:03<00:00, 150.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# GPT-2用のトークナイザを事前学習モデルからロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "max_length_input = 500\n",
    "\n",
    "# CarperAI/openai_summarize_tldrというデータセットをロード\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_tldr\", split=['train[:500]', 'valid[:500]'])\n",
    "\n",
    "# 訓練データと検証データに分ける\n",
    "train_set = [(sample[\"prompt\"], sample[\"label\"]) for sample in dataset[0]]\n",
    "val_set = [(sample[\"prompt\"], sample[\"label\"]) for sample in dataset[1]]\n",
    "\n",
    "# プロンプト（記事）とサマリー（ラベル）にデータを分割\n",
    "train_posts, train_summaries = zip(*train_set)\n",
    "val_posts, val_summaries = zip(*val_set)\n",
    "\n",
    "# サマリーを辞書に保存\n",
    "post_summary_dict = {}\n",
    "train_prompts = get_prompt_dataset(train_posts, max_length_input)\n",
    "for i in range(len(train_prompts)):\n",
    "    post_summary_dict[train_prompts[i]] = train_summaries[i]\n",
    "val_prompts = get_prompt_dataset(val_posts, max_length_input)\n",
    "for i in range(len(val_prompts)):\n",
    "    post_summary_dict[val_prompts[i]] = val_summaries[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35QMC3coxy-Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Initializing model: gpt2\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:d6ve6j8y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bd469460ce42dcb6eea60b55d0ccee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▅███▁</td></tr><tr><td>eval/loss</td><td>▆▃▁▂█</td></tr><tr><td>eval/runtime</td><td>█▆▂▁▄</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▇█▅</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▇█▅</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>█▇▅▃▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.42857</td></tr><tr><td>eval/loss</td><td>0.82701</td></tr><tr><td>eval/runtime</td><td>0.2244</td></tr><tr><td>eval/samples_per_second</td><td>31.192</td></tr><tr><td>eval/steps_per_second</td><td>4.456</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>train/global_step</td><td>50</td></tr><tr><td>train/learning_rate</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.5406</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.73378</td></tr><tr><td>train/train_runtime</td><td>117.2659</td></tr><tr><td>train/train_samples_per_second</td><td>10.66</td></tr><tr><td>train/train_steps_per_second</td><td>0.426</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-durian-10</strong> at: <a href='https://wandb.ai/seele/huggingface/runs/d6ve6j8y' target=\"_blank\">https://wandb.ai/seele/huggingface/runs/d6ve6j8y</a><br/> View job at <a href='https://wandb.ai/seele/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk5NzQzNzc4/version_details/v0' target=\"_blank\">https://wandb.ai/seele/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk5NzQzNzc4/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230922_035125-d6ve6j8y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:d6ve6j8y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4554ba2ad6d486b947aa0f7a37dee76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112757311113657, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/llm-seminar/notebooks/day6/wandb/run-20230922_035434-ayl47t6v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seele/trlx/runs/ayl47t6v' target=\"_blank\">ipykernel_launcher/gpt2/1gpu:master</a></strong> to <a href='https://wandb.ai/seele/trlx' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seele/trlx' target=\"_blank\">https://wandb.ai/seele/trlx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seele/trlx/runs/ayl47t6v' target=\"_blank\">https://wandb.ai/seele/trlx/runs/ayl47t6v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Starting training\n",
      "[RANK 0] Collecting rollouts\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.10/dist-packages/trlx/trainer/accelerate_ppo_trainer.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(score, dtype=torch.float, device=device).view(\n",
      "[RANK 0] Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfdae711e284a48a9136f15025e1be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[generation sweep 0/1 | eval batch 0/32]:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Computing rewards\n",
      "[RANK 0] Summarizing evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                         Evaluation #0 reward/mean: -0.077                                         </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> prompt                                            </span>┃<span style=\"font-weight: bold\"> output                                             </span>┃<span style=\"font-weight: bold\"> reward </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ SUBREDDIT: r/AskReddit                            │  I told my boyfriend and we've just done this.     │ -3.73  │\n",
       "│ TITLE: How do you get someone out of your head?   │                                                    │        │\n",
       "│ POST: Hi,                                         │ I've been very close to him. I love him deeply.    │        │\n",
       "│ I'm 22, and I have been with my girlfriend for 5  │                                                    │        │\n",
       "│ years now. We recently moved together. We've      │ You can be there for as long as you want.          │        │\n",
       "│ always loved each other intensely.                │                                                    │        │\n",
       "│                                                   │ Advertisements                                     │        │\n",
       "│ Problem, I recently started to have feelings for  │                                                    │        │\n",
       "│ an other person (a friend). This person has had a │                                                    │        │\n",
       "│ boyfriend for now 3 years, and has absolutely no  │                                                    │        │\n",
       "│ ideas. Those feelings were so strong, it was hard │                                                    │        │\n",
       "│ to hide them. After 2 months of me being distant  │                                                    │        │\n",
       "│ and really sad, my girlfriend forced me to say    │                                                    │        │\n",
       "│ what was bothering me. I'm not a good liar, and   │                                                    │        │\n",
       "│ now she knows.                                    │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ We decided to give us a week alone, I went to my  │                                                    │        │\n",
       "│ parents.                                          │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ Now, I'm completely lost. I keep on thinking      │                                                    │        │\n",
       "│ about this person, and I hate that. I would like  │                                                    │        │\n",
       "│ for those feelings to go away, to leave me alone. │                                                    │        │\n",
       "│ But I can't.                                      │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ What do I do? It's been 3 months now, and I'm     │                                                    │        │\n",
       "│ just desperate.                                   │                                                    │        │\n",
       "│ TL;DR:                                            │                                                    │        │\n",
       "├───────────────────────────────────────────────────┼────────────────────────────────────────────────────┼────────┤\n",
       "│ SUBREDDIT: r/pettyrevenge                         │  Gangnam Style was one of my first songs. It felt  │ -0.761 │\n",
       "│ TITLE: So, my mom woke me up with a loud TV.      │ so right, to this day, that I'm still able to hear │        │\n",
       "│ POST: She was in her living room, watching TV.    │ it from this day. What it wasn't listening to back │        │\n",
       "│ This was at about 8:30 in the morning, and she    │ then was anything special, and even more so than   │        │\n",
       "│ was exercising. She turned the TV up extra loud   │ the original                                       │        │\n",
       "│ to hear it over her excercycle, and woke me up. I │                                                    │        │\n",
       "│ went in there asking for her to turn it down. She │                                                    │        │\n",
       "│ said she didn't have to; I explained that I       │                                                    │        │\n",
       "│ always used headphones so she didn't have to deal │                                                    │        │\n",
       "│ with my noise and that she should give me a       │                                                    │        │\n",
       "│ little more respect, given that I paid rent at    │                                                    │        │\n",
       "│ the time.                                         │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ She disagreed. I went back to my room, rather     │                                                    │        │\n",
       "│ pissed off at the lack of equality. I had no lock │                                                    │        │\n",
       "│ on my door; but I had a dresser right next to it, │                                                    │        │\n",
       "│ so I pulled one of the drawers out enough so that │                                                    │        │\n",
       "│ it caused the door to not be openable. Then, I    │                                                    │        │\n",
       "│ turned my speakers up really loud and blasted     │                                                    │        │\n",
       "│ Gangnam Style on repeat, with the bass cranked up │                                                    │        │\n",
       "│ as high as it could go.                           │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ If you hate Gangnam Style for being overplayed,   │                                                    │        │\n",
       "│ you will see why I chose that particular song. I  │                                                    │        │\n",
       "│ personally don't mind it. But here's the thing    │                                                    │        │\n",
       "│ about my bass; it vibrates the walls, making one  │                                                    │        │\n",
       "│ hell of a lot of noise. Needless to say, my mom   │                                                    │        │\n",
       "│ was not pleased and shut off the internet. But it │                                                    │        │\n",
       "│ was oh so worth it.                               │                                                    │        │\n",
       "│ TL;DR:                                            │                                                    │        │\n",
       "├───────────────────────────────────────────────────┼────────────────────────────────────────────────────┼────────┤\n",
       "│ SUBREDDIT: r/relationships                        │  Don't ask for a shit or change your lifestyle or  │ -0.885 │\n",
       "│ TITLE: My girlfriend (20f) of two years cheated   │ make an appearance.                                │        │\n",
       "│ on me (20m) by kissing two guys at a Halloween    │                                                    │        │\n",
       "│ party.                                            │ Tags /subreddit/reddit-subreddit-reddit            │        │\n",
       "│ POST: Lately her and I have been having a few     │                                                    │        │\n",
       "│ problems, and these problems have been brought up │ This is in no way sponsored or endorsed by any of  │        │\n",
       "│ before a few times. One problem being that I      │ the moderators of Reddit                           │        │\n",
       "│ don't show enough affection. I don't tell her     │                                                    │        │\n",
       "│ she's pretty very often or don't compliment her   │                                                    │        │\n",
       "│ much. I feel terrible about it, but this time I   │                                                    │        │\n",
       "│ was really trying to change for her.              │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ For Halloween she went to visit her step brother  │                                                    │        │\n",
       "│ at a college and I got drunk with my friends and  │                                                    │        │\n",
       "│ watched movies. Last night (11/1) we got in a     │                                                    │        │\n",
       "│ huge fight about me not changing and how our      │                                                    │        │\n",
       "│ relationship won't work out and basically broke   │                                                    │        │\n",
       "│ up over the phone. So in an effort to try and fix │                                                    │        │\n",
       "│ it I drove to her house. She told me how at the   │                                                    │        │\n",
       "│ parties she went to that two guys kissed her. The │                                                    │        │\n",
       "│ first one she pushed away, but the second one I   │                                                    │        │\n",
       "│ asked her if she kissed him back and she said yes │                                                    │        │\n",
       "│ and that she did it because it made her feel      │                                                    │        │\n",
       "│ wanted, which I guess I haven't been making her   │                                                    │        │\n",
       "│ feel that way lately. We cried, we talked about   │                                                    │        │\n",
       "│ everything, we had great sex, and I stayed over   │                                                    │        │\n",
       "│ at her house just to sleep with her and then      │                                                    │        │\n",
       "│ snuck out in the morning so her parents wouldn't  │                                                    │        │\n",
       "│ know.                                             │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ We both obviously want to work things out but     │                                                    │        │\n",
       "│ aren't sure if we should. I love this girl, but   │                                                    │        │\n",
       "│ the more I think about it, all I can think about  │                                                    │        │\n",
       "│ is her cheating on me, and more importantly,      │                                                    │        │\n",
       "│ liking it. It makes me sick to my stomach. Should │                                                    │        │\n",
       "│ I even try to fix it or would I be better off     │                                                    │        │\n",
       "│ cutting all ties.                                 │                                                    │        │\n",
       "│ TL;DR:                                            │                                                    │        │\n",
       "└───────────────────────────────────────────────────┴────────────────────────────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                         Evaluation #0 reward/mean: -0.077                                         \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mprompt                                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutput                                            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mreward\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ SUBREDDIT: r/AskReddit                            │  I told my boyfriend and we've just done this.     │ -3.73  │\n",
       "│ TITLE: How do you get someone out of your head?   │                                                    │        │\n",
       "│ POST: Hi,                                         │ I've been very close to him. I love him deeply.    │        │\n",
       "│ I'm 22, and I have been with my girlfriend for 5  │                                                    │        │\n",
       "│ years now. We recently moved together. We've      │ You can be there for as long as you want.          │        │\n",
       "│ always loved each other intensely.                │                                                    │        │\n",
       "│                                                   │ Advertisements                                     │        │\n",
       "│ Problem, I recently started to have feelings for  │                                                    │        │\n",
       "│ an other person (a friend). This person has had a │                                                    │        │\n",
       "│ boyfriend for now 3 years, and has absolutely no  │                                                    │        │\n",
       "│ ideas. Those feelings were so strong, it was hard │                                                    │        │\n",
       "│ to hide them. After 2 months of me being distant  │                                                    │        │\n",
       "│ and really sad, my girlfriend forced me to say    │                                                    │        │\n",
       "│ what was bothering me. I'm not a good liar, and   │                                                    │        │\n",
       "│ now she knows.                                    │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ We decided to give us a week alone, I went to my  │                                                    │        │\n",
       "│ parents.                                          │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ Now, I'm completely lost. I keep on thinking      │                                                    │        │\n",
       "│ about this person, and I hate that. I would like  │                                                    │        │\n",
       "│ for those feelings to go away, to leave me alone. │                                                    │        │\n",
       "│ But I can't.                                      │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ What do I do? It's been 3 months now, and I'm     │                                                    │        │\n",
       "│ just desperate.                                   │                                                    │        │\n",
       "│ TL;DR:                                            │                                                    │        │\n",
       "├───────────────────────────────────────────────────┼────────────────────────────────────────────────────┼────────┤\n",
       "│ SUBREDDIT: r/pettyrevenge                         │  Gangnam Style was one of my first songs. It felt  │ -0.761 │\n",
       "│ TITLE: So, my mom woke me up with a loud TV.      │ so right, to this day, that I'm still able to hear │        │\n",
       "│ POST: She was in her living room, watching TV.    │ it from this day. What it wasn't listening to back │        │\n",
       "│ This was at about 8:30 in the morning, and she    │ then was anything special, and even more so than   │        │\n",
       "│ was exercising. She turned the TV up extra loud   │ the original                                       │        │\n",
       "│ to hear it over her excercycle, and woke me up. I │                                                    │        │\n",
       "│ went in there asking for her to turn it down. She │                                                    │        │\n",
       "│ said she didn't have to; I explained that I       │                                                    │        │\n",
       "│ always used headphones so she didn't have to deal │                                                    │        │\n",
       "│ with my noise and that she should give me a       │                                                    │        │\n",
       "│ little more respect, given that I paid rent at    │                                                    │        │\n",
       "│ the time.                                         │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ She disagreed. I went back to my room, rather     │                                                    │        │\n",
       "│ pissed off at the lack of equality. I had no lock │                                                    │        │\n",
       "│ on my door; but I had a dresser right next to it, │                                                    │        │\n",
       "│ so I pulled one of the drawers out enough so that │                                                    │        │\n",
       "│ it caused the door to not be openable. Then, I    │                                                    │        │\n",
       "│ turned my speakers up really loud and blasted     │                                                    │        │\n",
       "│ Gangnam Style on repeat, with the bass cranked up │                                                    │        │\n",
       "│ as high as it could go.                           │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ If you hate Gangnam Style for being overplayed,   │                                                    │        │\n",
       "│ you will see why I chose that particular song. I  │                                                    │        │\n",
       "│ personally don't mind it. But here's the thing    │                                                    │        │\n",
       "│ about my bass; it vibrates the walls, making one  │                                                    │        │\n",
       "│ hell of a lot of noise. Needless to say, my mom   │                                                    │        │\n",
       "│ was not pleased and shut off the internet. But it │                                                    │        │\n",
       "│ was oh so worth it.                               │                                                    │        │\n",
       "│ TL;DR:                                            │                                                    │        │\n",
       "├───────────────────────────────────────────────────┼────────────────────────────────────────────────────┼────────┤\n",
       "│ SUBREDDIT: r/relationships                        │  Don't ask for a shit or change your lifestyle or  │ -0.885 │\n",
       "│ TITLE: My girlfriend (20f) of two years cheated   │ make an appearance.                                │        │\n",
       "│ on me (20m) by kissing two guys at a Halloween    │                                                    │        │\n",
       "│ party.                                            │ Tags /subreddit/reddit-subreddit-reddit            │        │\n",
       "│ POST: Lately her and I have been having a few     │                                                    │        │\n",
       "│ problems, and these problems have been brought up │ This is in no way sponsored or endorsed by any of  │        │\n",
       "│ before a few times. One problem being that I      │ the moderators of Reddit                           │        │\n",
       "│ don't show enough affection. I don't tell her     │                                                    │        │\n",
       "│ she's pretty very often or don't compliment her   │                                                    │        │\n",
       "│ much. I feel terrible about it, but this time I   │                                                    │        │\n",
       "│ was really trying to change for her.              │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ For Halloween she went to visit her step brother  │                                                    │        │\n",
       "│ at a college and I got drunk with my friends and  │                                                    │        │\n",
       "│ watched movies. Last night (11/1) we got in a     │                                                    │        │\n",
       "│ huge fight about me not changing and how our      │                                                    │        │\n",
       "│ relationship won't work out and basically broke   │                                                    │        │\n",
       "│ up over the phone. So in an effort to try and fix │                                                    │        │\n",
       "│ it I drove to her house. She told me how at the   │                                                    │        │\n",
       "│ parties she went to that two guys kissed her. The │                                                    │        │\n",
       "│ first one she pushed away, but the second one I   │                                                    │        │\n",
       "│ asked her if she kissed him back and she said yes │                                                    │        │\n",
       "│ and that she did it because it made her feel      │                                                    │        │\n",
       "│ wanted, which I guess I haven't been making her   │                                                    │        │\n",
       "│ feel that way lately. We cried, we talked about   │                                                    │        │\n",
       "│ everything, we had great sex, and I stayed over   │                                                    │        │\n",
       "│ at her house just to sleep with her and then      │                                                    │        │\n",
       "│ snuck out in the morning so her parents wouldn't  │                                                    │        │\n",
       "│ know.                                             │                                                    │        │\n",
       "│                                                   │                                                    │        │\n",
       "│ We both obviously want to work things out but     │                                                    │        │\n",
       "│ aren't sure if we should. I love this girl, but   │                                                    │        │\n",
       "│ the more I think about it, all I can think about  │                                                    │        │\n",
       "│ is her cheating on me, and more importantly,      │                                                    │        │\n",
       "│ liking it. It makes me sick to my stomach. Should │                                                    │        │\n",
       "│ I even try to fix it or would I be better off     │                                                    │        │\n",
       "│ cutting all ties.                                 │                                                    │        │\n",
       "│ TL;DR:                                            │                                                    │        │\n",
       "└───────────────────────────────────────────────────┴────────────────────────────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9826abf152664372ba317a61a4355b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Collecting rollouts\n",
      "[RANK 0] Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0d04744ff742a3ae97cefe307f7f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[generation sweep 0/1 | eval batch 0/32]:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = TRLConfig(\n",
    "    train=TrainConfig(\n",
    "        seq_length=550,\n",
    "        epochs=50,\n",
    "        total_steps=100000,\n",
    "        batch_size=4,\n",
    "        checkpoint_interval=10000,\n",
    "        eval_interval=200,\n",
    "        pipeline=\"PromptPipeline\",\n",
    "        trainer=\"AcceleratePPOTrainer\",\n",
    "    ),\n",
    "    model=ModelConfig(\n",
    "        model_path=\"gpt2\",\n",
    "        num_layers_unfrozen=8,\n",
    "    ),\n",
    "    tokenizer=TokenizerConfig(\n",
    "        tokenizer_path=\"gpt2\",\n",
    "        truncation_side=\"right\",\n",
    "    ),\n",
    "    optimizer=OptimizerConfig(\n",
    "        name=\"adamw\",\n",
    "        kwargs={\n",
    "            \"lr\": 5.0e-6,\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1.0e-8,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "    ),\n",
    "    scheduler=SchedulerConfig(\n",
    "        name=\"cosine_annealing\",\n",
    "        kwargs={\n",
    "            \"T_max\": 100000,\n",
    "            \"eta_min\": 5.0e-6,\n",
    "        },\n",
    "    ),\n",
    "    method=PPOConfig(\n",
    "        name=\"PPOConfig\",\n",
    "        num_rollouts=128,\n",
    "        chunk_size=16,\n",
    "        ppo_epochs=4,\n",
    "        init_kl_coef=0.1,\n",
    "        target=6,\n",
    "        horizon=10000,\n",
    "        gamma=1,\n",
    "        lam=0.95,\n",
    "        cliprange=0.2,\n",
    "        cliprange_value=0.2,\n",
    "        vf_coef=0.2,\n",
    "        scale_reward=None,\n",
    "        ref_mean=None,\n",
    "        ref_std=None,\n",
    "        cliprange_reward=10,\n",
    "        gen_kwargs={\n",
    "            \"max_new_tokens\": 50,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = trlx.train(\n",
    "    reward_fn=reward_fn,\n",
    "    prompts=train_prompts,\n",
    "    eval_prompts=val_prompts[0:500],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmg4StF22zrO"
   },
   "source": [
    "## 5. 参考文献\n",
    "- [Implementing RLHF: Learning to Summarize with trlX](https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2)\n",
    "\n",
    "- [General overview about RLHF](https://huggingface.co/blog/rlhf)\n",
    "- [Another end-to-end example with trlX](https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2)\n",
    "- [Similar human-in-the-loop annotation framework](https://github.com/CarperAI/cheese/tree/main/examples)\n",
    "- [Antropic harmless RLHF paper](https://arxiv.org/pdf/2204.05862.pdf) and [blog about CAI general principles](https://lifearchitect.ai/anthropic/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
