{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+KmDMsKdj14TZ1VOE6Y9+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# サマースクール LLM　第6回 宿題　クイズ"],"metadata":{"id":"-4e8DU8r2uAM"}},{"cell_type":"markdown","source":["## 課題\n","今回の課題は確認クイズです。\n","\n","上から回答していくとcsv提出用csv `submission_pred.csv`が作成されます。\n","提出期限までにomnicampusに提出してください。\n","\n","**採点は締め切り後に一回だけ行われます。**\n","（**即時採点ではありません。**）"],"metadata":{"id":"mKNW62di2ypv"}},{"cell_type":"code","source":["# この部分は修正しないでください\n","import numpy as np\n","import pandas as pd\n","\n","NUM_EXAMPLE_QUESTIONS = 1\n","example = np.zeros(NUM_EXAMPLE_QUESTIONS,int)\n","\n","NUM_QUESTIONS = 5\n","myanswer = np.zeros(NUM_QUESTIONS,int)"],"metadata":{"id":"-WUrWAL02v28"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```python\n","example[0] = 2\n","\n","```"],"metadata":{"id":"deiGxSVk2-dG"}},{"cell_type":"markdown","source":["### 確認クイズ\n","**Q1 RLHFを行う目的として最も適切なものを1つ選択してください。**\n","\n","1. 人間のフィードバックをモデルに与えることで、モデルが暴走するのを防ぐため\n","2. 人間の価値観や意図に沿った出力になるようにモデルを学習するため\n","3. モデルに新しい知識を追加して、有用性を高めるため\n","4. モデルの出力のフォーマットを調整するため\n","\n","回答: $\\fbox{　(0)　}$\n","\n","\n","**Q2 Alignmnetの基準に関する次の説明のうち間違っているものを1つ選択してください。**\n","\n","1. Helpfulとは、ユーザーの質問に対してできるだけ簡潔で有用な回答を行うという意図を考慮した基準であり、TruthfulQAなどの評価データが存在する\n","2. Honestとは、情報の虚偽がなく、正確な文章を回答する様な意図を考慮した基準であり、HaluEvalなどの評価データが存在する\n","3. Harmlessとは、攻撃的、暴力的なことを回答しない様にするという意図を考慮した基準であり、SHP(Stanford Human Preferences Dataset)などの評価データが存在する\n","4. Helpfulとは、相手のレベルに合わせた質問応答を行ったり、不足情報がある場合に，適切な質問を投げかけて情報を引き出すように回答を行う様な意図を考慮した基準であり、HH-RLHFなどの評価データが存在する\n","\n","回答: $\\fbox{　(1)　}$\n","\n","\n","**Q3 InstructGPTの学習ステップとして間違っているものを1つ選択してください。**\n","1. プロンプトのデータセットを用意し、そのプロンプトに対する人間のlabelerの回答を元に教師あり学習を行う\n","2. あるプロンプトに対するSFTモデルの出力を複数集め、その出力に関する「好ましさ」を、人間のlabelerがランクづけする\n","3. 報酬モデルの学習とベースモデルの教師あり学習は交互に行い、その手順を何度も繰り返す\n","4. あるプロンプトに対するSFTモデルの出力に対して、報酬モデルが報酬を生成し、PPOによる強化学習を行う\n","\n","回答: $\\fbox{　(2)　}$\n","\n","\n","**Q4 PPO-ptxにおいて、Alignment Tax(アライメント税)に対する解決策として最も有効なものを1つ選択してください。**\n","\n","1. Reward Scaling\n","2. KL Penalty\n","3. GAE\n","4. Replay\n","\n","回答: $\\fbox{　(3)　}$\n","\n","\n","**Q5 次のRLHFの発展的手法のうち、報酬モデルを直接用いない手法を1つ選択してください。**\n","\n","1. RLCD(Reinforcement Learning from Contrast Distillation for Language Model Alignment)\n","2. DPO(Direct Preference Optimization)\n","3. RRHF(Rank Responses to Align Language Models with Human Feedback without tears)\n","4. RAFT(Reward rAnked FineTuning for Generative Foundation Model Alignment)\n","\n","回答: $\\fbox{　(4)　}$\n","\n","\n","\n","\n"],"metadata":{"id":"OVTWSkgr3Bgh"}},{"cell_type":"code","source":["myanswer[0] = # WRITE ME\n","myanswer[1] = # WRITE ME\n","myanswer[2] = # WRITE ME\n","myanswer[3] = # WRITE ME\n","myanswer[4] = # WRITE ME"],"metadata":{"id":"VQZnoNQW3B64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# omnicampusへ提出するcsvファイルが作成されます。\n","if not np.any(myanswer==0):\n","    submission = pd.Series(myanswer, name='label')\n","    submission.to_csv('/content/submission_pred.csv', header=True, index_label='id')\n","else:\n","    print('Error: please answer all of the questions')"],"metadata":{"id":"WXzIgODm3Hrl"},"execution_count":null,"outputs":[]}]}