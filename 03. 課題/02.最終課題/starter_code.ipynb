{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大規模言語モデル講座 最終課題 LLMコンペティション\n",
    "下記の３種類のベンチマーク性能を高めよ。ただし、事前学習、FinetuningやRLHF等、プロンプティングの工夫など、授業で学んだことを自由に利用して構わない。モデルや学習に利用できるデータについてはルールを参照のこと。\n",
    "- Type１：日本語QA\n",
    "    - 日本に関する知識を答える選択肢問題\n",
    "    - 出力は選択肢番号(1~5)を出力してください。(int型)\n",
    "    - 評価は正解率で行います。\n",
    "- Type２：文章要約\n",
    "    - 出力は要約文を出力してください。(str型)\n",
    "    - 評価はROUGE-2 score(F1 score)で行います。\n",
    "- Type３：Instruction Following\n",
    "    - 与えられた指示に対して適切な出力を返してください。\n",
    "    - 出力は生成文を出力してください。(str型)\n",
    "    - 評価は人手 or 外部の大規模言語モデルを用いて行います。\n",
    "    - ※本評価に関しては、Type1・Type2ベンチマークでの成績上位者を対象に行います。\n",
    "        - Type1・Type2成績上位者のうち、Type3のコンペに参加希望の方には人手による評価に協力いただきます\n",
    "\n",
    "## https://docs.google.com/document/d/1ZyURrEZ-qQulQ3gEQBf6IWHdSTZyU7d-mDAukqzLL-M/edit?usp=sharing  \n",
    "上記リンク先ドキュメントから特に重要な注意事項を抜粋\n",
    "- 修了要件の1つとして含まれる\n",
    "- 開催時期: 2023/09/25 - 2023/10/10 23:59（予定）\n",
    "    - 締切は2023/10/10 23:59までです。提出の際は以下のファイルをOmnicampusへ提出してもらいます。\n",
    "        - 提出物１：学習・推論コード(train_and_predict.ipynb)\n",
    "            - 学習データを作成、読み込んで学習させるコード、評価データを読み込み推論結果のjsonファイルを出力するコード\n",
    "            - 結果が再現されるか確認に使用します。\n",
    "            - 成績優秀者の提出コードはコンペ終了後、他の受講生に対して公開させていただきます。\n",
    "            - starter_code.ipynbを参考にしてください。\n",
    "        - 提出物２：推論結果(submission.json)\n",
    "            - id, task_type, text, answerのkeyがあることを確認してください(正しく入っていない場合、スコア付けが行われません)\n",
    "- テストデータはtest.jsonというデータをコンペ開始とともに配布します、そのデータにType1, Type2, Type3のデータが全て含まれています\n",
    "    - 注意: テストデータはコンペ期間中に追加される可能性があります、追加した際はslackにてアナウンスを行うのでご確認ください。\n",
    "- 配布したGPUリソース(50GPU時間以内)で学習・推論を終えてください\n",
    "    - なお、GPU時間には最終的な提出物の推論時間を含みます。推論のためのGPU時間を確保しておいてください。※軽いモデルや軽いデータで学習を終えて、1度提出の流れまでは確認することを推奨します。\n",
    "- ご自身で用意したGPUリソースで学習・試行錯誤をすることは可能ですが、提出する結果の元となるモデルは、配布したGPUリソースで学習を終えてください。\n",
    "    - 簡単な動作確認などはgoogle colabなどを活用し、大規模な学習をomnicampus上で行うことをオススメします。  \n",
    "- /workspace/assetsというディレクトリ以下は、インスタンスを閉じた後、再度立ち上げても作業中のファイルやダウンロードや学習させたモデルなどが残るようになっております。\n",
    "- 受講生間での議論について\n",
    "    - 生成した出力結果自体を共有するのはやめてください。\n",
    "    - コンペに関しての受講生同士のディスカッションは#コンペ_受講者ディスカッションで行ってください。\n",
    "    - コンペのルールに関してご不明な点があれば#コンペ_運営へのルール確認で質問してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code\n",
    "testデータを読み込み推論し、提出するファイルを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの確認、枚数に応じて割り当てられた計算資源の消費速度が異なるのでお気をつけください。コンペ期間中は2時間で自動でインスタンスが落ちず、無制限となるため、インスタンスの停止し忘れにもご注意ください。\n",
    "# 50GPU時間を超えると、インスタンスを立ち上げることができませんのでご注意ください。\n",
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "# /workspace/assets以下のファイルは永続化されます。作業中のcodeや学習した重みやデータはこちらに保存してください。(容量数TB、IOは遅い)\n",
    "# /workspace/assets以外の部分のデータ容量は100GBまでです。複数モデルをダウンロードしている場合は容量にご注意ください。\n",
    "# 具体的には以下のフォルダを適宜消す、あるいは/workspace/assets以下に移動するなどしてください。\n",
    "os.environ['HF_HOME'] = '/workspace/hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインストール(omnicampus上の環境にないライブラリをインストールした方のみ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・推論コード(train_and_predict.ipynb)の実行に必要なライブラリをインストールする\n",
    "# pip install new_library==version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論し、提出するファイルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# sampleデータ(学習・promptに利用して良いデータ)の確認\n",
    "sample_data = json.load(open('sample.json', 'r'))\n",
    "print(len(sample_data))\n",
    "print(sample_data[:5])\n",
    "# 推論を行うデータの確認、テストデータはコンペ期間中に追加される可能性があります、追加した際はslackにてアナウンスを行うのでご確認ください。\n",
    "test_data = json.load(open('test.json', 'r'))\n",
    "print(len(test_data))\n",
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのダウンロードと読み込み\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "model_name = 'matsuo-lab/weblab-10b-instruction-sft'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "seed = 12\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "start_time = time.time()\n",
    "# モデル・GPUメモリによって扱える最大トークン数が異なるので注意\n",
    "# タスクによっては入力文が長いものがあります、どのように対処するかは各自で工夫いただければと思います。\n",
    "# token長の2乗に計算量とメモリが必要になるので、提出用の推論処理のGPU資源確保の際にはご注意ください。\n",
    "# max_length = 2048\n",
    "max_length = 256\n",
    "test_data = json.load(open('test.json', 'r'))\n",
    "for data in test_data:\n",
    "    if data['task_type'] == 'generation':\n",
    "        text = f\"{data['text']} \\n\\n\\n回答:\"\n",
    "        # print(text)\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        max_new_token =int(max_length / 8)\n",
    "        # print(token_ids.shape)\n",
    "        if token_ids.shape[1] > max_length:\n",
    "            token_ids = token_ids[:, -(max_length-max_new_token-1):]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                token_ids.to(model.device),\n",
    "                max_length=max_length,\n",
    "                do_sample=False,\n",
    "                # do_sample=True,\n",
    "                # temperature=0.7,\n",
    "                # top_p=0.95\n",
    "            )\n",
    "        print(tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True))\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][len(token_ids[0]):], skip_special_tokens=True)\n",
    "        data['answer'] = output\n",
    "    \n",
    "    elif data['task_type'] == 'summarization':\n",
    "        text = f\"\"\"{data['text']}\\n\\n\\n上記の文章を要約してください。要約:\"\"\"\n",
    "        # print(text)\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        max_new_token =int(max_length / 8)\n",
    "        # print(token_ids.shape)\n",
    "        if token_ids.shape[1] > max_length:\n",
    "            token_ids = token_ids[:, -(max_length-max_new_token-1):]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                token_ids.to(model.device),\n",
    "                max_length=max_length,\n",
    "                do_sample=False,\n",
    "                # do_sample=True,\n",
    "                # temperature=0.7,\n",
    "                # top_p=0.95\n",
    "            )\n",
    "        print(tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True))\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][len(token_ids[0]):], skip_special_tokens=True)\n",
    "        data['answer'] = output\n",
    "    \n",
    "    elif data['task_type'] == 'multiple_choice':\n",
    "        text = f\"[問題]:{data['text']} \\n\\n[選択肢]:[{data['choices'][0]['choice_id']}. {data['choices'][0]['text']}, {data['choices'][1]['choice_id']}. {data['choices'][1]['text']}, {data['choices'][2]['choice_id']}. {data['choices'][2]['text']}, {data['choices'][3]['choice_id']}. {data['choices'][3]['text']}, {data['choices'][4]['choice_id']}. {data['choices'][4]['text']}] \\n\\n[答えの選択肢番号]:\"\n",
    "        # print(text)\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                token_ids.to(model.device),\n",
    "                max_length=max_length,\n",
    "                do_sample=False,\n",
    "                # do_sample=True,\n",
    "                # temperature=0.7,\n",
    "                # top_p=0.95\n",
    "            )\n",
    "\n",
    "        print(tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True))\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][len(token_ids[0]):], skip_special_tokens=True)\n",
    "        # 出力が1~5の数字になるようにする\n",
    "        data['answer'] = next((int(char) for char in output if char in '12345'), random.randint(1, 5))\n",
    "print('推論にかかった秒数', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論結果の保存、id, task_type, text, answerのkeyがあることを確認してください(正しく入っていない場合、スコア付けが行われません)\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=4)\n",
    "\n",
    "check_submission_data = json.load(open('submission.json', 'r'))\n",
    "print(len(check_submission_data), check_submission_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スコア改善のアドバイス\n",
    "- 他のモデルを使用してみる\n",
    "    - 上記のサンプルコードで使用しているモデルは[10Bパラメータのモデル](https://huggingface.co/matsuo-lab/weblab-10b-instruction-sft)です、これよりも大きいモデルや別のデータでinstruction-tuning/RLHF学習済みのモデルを使用してみると性能が向上するかもしれません。\n",
    "        - 無料で使用できるGoogle Colabでもモデルの動作確認を行えるモデルがあるため、重たい計算の必要のない試行錯誤はlocal環境やGoogle Colabで行うことをオススメします。\n",
    "- promptingを工夫する\n",
    "    - 現状はシンプルなpromptingになっています。選択したモデルの出力がより良くなるようにfew-shot promptingなどpromptingを工夫してみましょう。\n",
    "- decodingを工夫しましょう\n",
    "    - タスクに応じてdecoding手法(greedy-decoding, beam-searchなど)を変更すると性能が向上するかもしれません。\n",
    "    - 選択肢問題の場合は選択肢を選ぶ際の工夫も有効的かもしれません。\n",
    "        - https://huggingface.co/blog/evaluating-mmlu-leaderboard\n",
    "- 学習を工夫しましょう\n",
    "    - 事前学習・instruction tuning・RLHF学習などを学習データを工夫して行い更なる性能向上を目指しましょう。\n",
    "        - Day4, Day5, Day6の演習コードや以下のリンクも参考にしてみてください。\n",
    "            - https://github.com/facebookresearch/llama-recipes\n",
    "            - https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications\n",
    "- 受講生同士で知見を共有しましょう\n",
    "    - submission.jsonの共有は認めておりませんが、データ・実装の共有・prompting・モデル選択の工夫など是非受講生間で議論いただければと思います。\n",
    "    - コンペに関しての受講生同士のディスカッションは#コンペ_受講者ディスカッションで行ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数GPUによる学習の例\n",
    "詳細は以下のリンクを参考にしてください\n",
    "- https://huggingface.co/docs/transformers/accelerate\n",
    "- https://huggingface.co/docs/transformers/main_classes/deepspeed\n",
    "- https://pytorch.org/tutorials/distributed/home.html\n",
    "- https://github.com/huggingface/peft/tree/main/examples/causal_language_modeling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの作成\n",
    "import json\n",
    "sample_data = json.load(open('sample.json', 'r'))\n",
    "training_data = dict(data=[])\n",
    "\n",
    "for data in sample_data:\n",
    "    if data['task_type'] == 'generation':\n",
    "        text = f\"{data['text']} \\n\\n\\n回答:{data['answer']}\"\n",
    "        training_data['data'].append(text)\n",
    "    elif data['task_type'] == 'summarization':\n",
    "        text = f\"\"\"{data['text']}\\n\\n\\n上記の文章を要約してください。要約:{data['answer']}\"\"\"\n",
    "        training_data['data'].append(text)\n",
    "    elif data['task_type'] == 'multiple_choice':\n",
    "        text = f\"[問題]:{data['text']} \\n\\n[選択肢]:[{data['choices'][0]['choice_id']}. {data['choices'][0]['text']}, {data['choices'][1]['choice_id']}. {data['choices'][1]['text']}, {data['choices'][2]['choice_id']}. {data['choices'][2]['text']}, {data['choices'][3]['choice_id']}. {data['choices'][3]['text']}, {data['choices'][4]['choice_id']}. {data['choices'][4]['text']}] \\n\\n[答えの選択肢番号]:{data['answer']}\"\n",
    "        training_data['data'].append(text)\n",
    "    else:\n",
    "        pass\n",
    "training_data\n",
    "# 学習データの保存、/workspace/assets以下に保存することで永続化されます。\n",
    "with open('/workspace/assets/training_data.json', 'w') as f:\n",
    "    json.dump(training_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class FinetuningDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=256):\n",
    "        self.model_inputs = []\n",
    "\n",
    "        for data in data_dict['data']:\n",
    "            encodings_dict = tokenizer(data)\n",
    "            # 長い文章を要約するときにどうデータを処理するか、ここでは一旦無視\n",
    "            if len(encodings_dict['input_ids']) > max_length:\n",
    "                continue\n",
    "            else:\n",
    "                model_inputs = tokenizer(data, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "                del model_inputs['token_type_ids']\n",
    "                # ここでは全文を予測対象としている(事前学習と同じ設定)\n",
    "                # instruction tuningの場合はinstruction後の出力を予測対象とするので注意(例: https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py#L112, SFTTrainer https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)\n",
    "                labels = tokenizer(data, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "                labels = labels[\"input_ids\"]\n",
    "                labels[labels == tokenizer.pad_token_id] = -100\n",
    "                model_inputs[\"labels\"] = labels\n",
    "                self.model_inputs.append(model_inputs)\n",
    "    def __len__(self):\n",
    "        return len(self.model_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.model_inputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import random\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\", inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "model_name = 'matsuo-lab/weblab-10b-instruction-sft'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "seed = 12\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, default_data_collator, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "batch_size = 1\n",
    "lr = 5e-4\n",
    "num_epochs = 3\n",
    "\n",
    "dataset = FinetuningDataset(training_data, tokenizer)\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(model.device).squeeze(0) for k, v in batch.items()}\n",
    "        print(\"input_ids shape:\", batch[\"input_ids\"].shape)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for step, batch in enumerate(tqdm(val_dataloader)):\n",
    "        batch = {k: v.to(model.device).squeeze(0) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(val_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "\n",
    "print('学習にかかった秒数', time.time() - start_time)\n",
    "# 学習済みモデルの保存\n",
    "model.save_pretrained('/workspace/assets/my_awesome_model_00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "seed = 12\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "start_time = time.time()\n",
    "# モデル・GPUメモリによって扱える最大トークン数が異なるので注意\n",
    "# タスクによっては入力文が長いものがあります、どのように対処するかは各自で工夫いただければと思います。\n",
    "# token長の2乗に計算量とメモリが必要になるので、提出用の推論処理のGPU資源確保の際にはご注意ください。\n",
    "# max_length = 2048\n",
    "max_length = 256\n",
    "test_data = json.load(open('test.json', 'r'))\n",
    "for data in test_data:\n",
    "    if data['task_type'] == 'generation':\n",
    "        text = f\"{data['text']} \\n\\n\\n回答:\"\n",
    "        # print(text)\n",
    "        inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "        del inputs['token_type_ids']\n",
    "        max_new_token =int(max_length / 8)\n",
    "        if inputs.input_ids.shape[1] > max_length:\n",
    "            inputs.input_ids = inputs.input_ids[:, -(max_length-max_new_token-1):]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                do_sample=False,\n",
    "                # do_sample=True,\n",
    "                # temperature=0.7,\n",
    "                # top_p=0.95\n",
    "            )\n",
    "        print(tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True))\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][len(inputs.token_ids[0]):], skip_special_tokens=True)\n",
    "        data['answer'] = output\n",
    "    \n",
    "    elif data['task_type'] == 'summarization':\n",
    "        text = f\"\"\"{data['text']}\\n\\n\\n上記の文章を要約してください。要約:\"\"\"\n",
    "        # print(text)\n",
    "        inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "        del inputs['token_type_ids']\n",
    "        max_new_token =int(max_length / 8)\n",
    "        if inputs.input_ids.shape[1] > max_length:\n",
    "            inputs.input_ids = inputs.input_ids[:, -(max_length-max_new_token-1):]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                do_sample=False,\n",
    "                # do_sample=True,\n",
    "                # temperature=0.7,\n",
    "                # top_p=0.95\n",
    "            )\n",
    "        print(tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True))\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][len(inputs.token_ids[0]):], skip_special_tokens=True)\n",
    "        data['answer'] = output\n",
    "    \n",
    "    elif data['task_type'] == 'multiple_choice':\n",
    "        text = f\"[問題]:{data['text']} \\n\\n[選択肢]:[{data['choices'][0]['choice_id']}. {data['choices'][0]['text']}, {data['choices'][1]['choice_id']}. {data['choices'][1]['text']}, {data['choices'][2]['choice_id']}. {data['choices'][2]['text']}, {data['choices'][3]['choice_id']}. {data['choices'][3]['text']}, {data['choices'][4]['choice_id']}. {data['choices'][4]['text']}] \\n\\n[答えの選択肢番号]:\"\n",
    "        # print(text)\n",
    "        inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "        del inputs['token_type_ids']\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                do_sample=False,\n",
    "                # do_sample=True,\n",
    "                # temperature=0.7,\n",
    "                # top_p=0.95\n",
    "            )\n",
    "\n",
    "        print(tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True))\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "        # 出力が1~5の数字になるようにする\n",
    "        data['answer'] = next((int(char) for char in output if char in '12345'), random.randint(1, 5))\n",
    "print('推論にかかった秒数', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論結果の保存、id, task_type, text, answerのkeyがあることを確認してください(正しく入っていない場合、スコア付けが行われません)\n",
    "with open('trained_model_submission.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=4)\n",
    "\n",
    "check_submission_data = json.load(open('trained_model_submission.json', 'r'))\n",
    "print(len(check_submission_data), check_submission_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
